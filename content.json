{"meta":{"title":"League of Data","subtitle":null,"description":"HOW NOT TO BE WRONG ?","author":"Xiao蜗牛","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-07-16T15:33:53.972Z","updated":"2017-07-16T15:33:53.972Z","comments":true,"path":"about/about.html","permalink":"http://yoursite.com/about/about.html","excerpt":"","text":"blank"},{"title":"关于","date":"2017-07-13T15:07:37.000Z","updated":"2017-07-13T15:11:50.432Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-07-13T15:05:02.000Z","updated":"2017-07-13T15:12:45.481Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-07-13T15:07:18.000Z","updated":"2017-07-13T15:12:14.966Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"基于gensim的Wiki百科中文word2vec训练","slug":"01-word2vec-based-on-gensim","date":"2017-07-13T07:30:16.000Z","updated":"2017-07-16T16:45:46.916Z","comments":true,"path":"2017/07/13/01-word2vec-based-on-gensim/","link":"","permalink":"http://yoursite.com/2017/07/13/01-word2vec-based-on-gensim/","excerpt":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。","text":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。 我们怎么得到一个词的word2vec呢？下面我们将介绍如何使用python gensim得到我们想要的词向量。总的来说，包括以下几个步骤： wiki中文数据预处理 文本数据分词 gensim word2vec训练 wiki中文数据预处理首先，下载wiki中文数据：zhwiki-latest-pages-articles.xml.bz2。因为zhwiki数据中包含很多繁体字，所以我们想获得简体语料库，接下来需要做以下两件事： 使用gensim模块中的WikiCorpus从bz2中获取原始文本数据 使用OpenCC将繁体字转换为简体字 WikiCorpus获取原始文本数据数据处理的python代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from __future__ import print_functionfrom gensim.corpora import WikiCorpusimport jiebaimport codecsimport osimport sixfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport multiprocessing class Config: data_path = &apos;xxx/zhwiki&apos; zhwiki_bz2 = &apos;zhwiki-latest-pages-articles.xml.bz2&apos; zhwiki_raw = &apos;zhwiki_raw.txt&apos; zhwiki_raw_t2s = &apos;zhwiki_raw_t2s.txt&apos; zhwiki_seg_t2s = &apos;zhwiki_seg.txt&apos; embedded_model_t2s = &apos;embedding_model_t2s/zhwiki_embedding_t2s.model&apos; embedded_vector_t2s = &apos;embedding_model_t2s/vector_t2s&apos; def dataprocess(_config): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) wiki = WikiCorpus(os.path.join(_config.data_path, _config.zhwiki_bz2), lemmatize=False, dictionary=&#123;&#125;) for text in wiki.get_texts(): if six.PY3: output.write(b&apos; &apos;.join(text).decode(&apos;utf-8&apos;, &apos;ignore&apos;) + &apos;\\n&apos;) else: output.write(&apos; &apos;.join(text) + &apos;\\n&apos;) i += 1 if i % 10000 == 0: print(&apos;Saved &apos; + str(i) + &apos; articles&apos;) output.close() print(&apos;Finished Saved &apos; + str(i) + &apos; articles&apos;)config = Config()dataprocess(config) 使用OpenCC将繁体字转换为简体字这里，需要预先安装OpenCC，关于OpenCC在linux环境中的安装方法，请参考这篇文章。仅仅需要两行linux命令就可以完成繁体字转换为简体字的认为，而且速度很快。12$ cd /xxx/zhwiki/$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json 文本数据分词对于分词这个任务，我们直接使用了python的jieba分词模块。你也可以使用哈工大的ltp或者斯坦福的nltk python接口进行分词，准确率及权威度挺高的。不过这两个安装的时候会花费很长时间，尤其是斯坦福的。关于jieba的分词处理代码，参考如下：1234567891011121314151617181920212223242526272829303132def is_alpha(tok): try: return tok.encode(&apos;ascii&apos;).isalpha() except UnicodeEncodeError: return Falsedef zhwiki_segment(_config, remove_alpha=True): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) print(&apos;Start...&apos;) with codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw_t2s), &apos;r&apos;, encoding=&apos;utf-8&apos;) as raw_input: for line in raw_input.readlines(): line = line.strip() i += 1 print(&apos;line &apos; + str(i)) text = line.split() if True: text = [w for w in text if not is_alpha(w)] word_cut_seed = [jieba.cut(t) for t in text] tmp = &apos;&apos; for sent in word_cut_seed: for tok in sent: tmp += tok + &apos; &apos; tmp = tmp.strip() if tmp: output.write(tmp + &apos;\\n&apos;) output.close()zhwiki_segment(config) gensim word2vec训练python的gensim模块提供了word2vec训练，为我们模型的训练提供了很大的方便。关于gensim的使用方法，可以参考基于Gensim的Word2Vec实践。本次训练的词向量大小size为50，训练窗口为5，最小词频为5，并使用了多线程，具体代码如下：123456789101112131415161718192021def word2vec(_config, saved=False): print(&apos;Start...&apos;) model = Word2Vec(LineSentence(os.path.join(_config.data_path, _config.zhwiki_seg_t2s)), size=50, window=5, min_count=5, workers=multiprocessing.cpu_count()) if saved: model.save(os.path.join(_config.data_path, _config.embedded_model_t2s)) model.save_word2vec_format(os.path.join(_config.data_path, _config.embedded_vector_t2s), binary=False) print(&quot;Finished!&quot;) return model def wordsimilarity(word, model): semi = &apos;&apos; try: semi = model.most_similar(word, topn=10) except KeyError: print(&apos;The word not in vocabulary!&apos;) for term in semi: print(&apos;%s,%s&apos; % (term[0],term[1]))model = word2vec(config, saved=True) word2vec训练已经完成，我们得到了想要的模型以及词向量，并保存到本地。下面我们分别查看同【宝马】和【腾讯】最相近的前10个词语。可以发现：和【宝马】相近的词大都属于汽车行业，而且是汽车品牌；和【腾讯】相近的词大都属于互联网行业。1234567891011121314151617181920212223&gt;&gt;&gt; wordsimilarity(word=u&apos;宝马&apos;, model=model)保时捷,0.92567974329固特异,0.888278841972劳斯莱斯,0.884045600891奥迪,0.881808757782马自达,0.881799697876亚菲特,0.880708634853欧宝,0.877104878426雪铁龙,0.876984715462玛莎拉蒂,0.868475496769桑塔纳,0.865387916565&gt;&gt;&gt; wordsimilarity(word=u&apos;腾讯&apos;, model=model)网易,0.880213916302优酷,0.873666107655腾讯网,0.87026232481广州日报,0.859486758709微信,0.835543811321天涯社区,0.834927380085李彦宏,0.832848489285土豆网,0.831390202045团购,0.829696238041搜狐网,0.825544642448 附相关数据及代码，包含：简体字转换后文本，分词后文本，以及50维word2vec词向量。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/categories/自然语言处理/"}],"tags":[{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"},{"name":"gensim","slug":"gensim","permalink":"http://yoursite.com/tags/gensim/"},{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"}]},{"title":"OpenCC - 简体繁体转换","slug":"02-opencc-tools","date":"2017-07-12T07:30:16.000Z","updated":"2017-07-13T16:01:15.889Z","comments":true,"path":"2017/07/12/02-opencc-tools/","link":"","permalink":"http://yoursite.com/2017/07/12/02-opencc-tools/","excerpt":"最近使用中文维基百科数据训练Word2Vec时，发现数据里面包含了很多繁体字，这就很尴尬了。这时候就知道OpenCC的强大了。哈哈，本来打算直接使用python里面的opencc模块的，但是在安装，编译opencc时遇到了各种错误。花费了很长时间，终于安装成功，但是文本处理起来效率很低。最终选择了直接在linux下安装OpenCC，处理的效率着实安慰了我受伤的心 – 很快，特别快。","text":"最近使用中文维基百科数据训练Word2Vec时，发现数据里面包含了很多繁体字，这就很尴尬了。这时候就知道OpenCC的强大了。哈哈，本来打算直接使用python里面的opencc模块的，但是在安装，编译opencc时遇到了各种错误。花费了很长时间，终于安装成功，但是文本处理起来效率很低。最终选择了直接在linux下安装OpenCC，处理的效率着实安慰了我受伤的心 – 很快，特别快。 好啦，接下来总结下OpenCC的安装方法，万一以后又用到它了呢？主要参考这篇博客： 检查下linux环境下是否已经安装cmake以及git，如果没有，那就通过yum安装好。 12$ yum install cmake$ yum install git 克隆下OpennCC开源项目OpennCC开源项目。 1$ git clone https://github.com/BYVoid/OpenCC 编译OpenCC 123$ cd OpenCC$ make$ make install 创建libopencc.so.2链接 如果不知道libopencc.so.2的路径，可以通过find / -name libopencc.so.2查找。1$ ln -s /usr/lib/libopencc.so.2 /usr/lib64/libopencc.so.2 通过查看 OpenCC 版本，检查OpenCC是否已经安装成功 1$ opencc --version 测试用例 12345678# 繁体转简体$ echo &apos;歐幾里得 西元前三世紀的希臘數學家&apos; | opencc -c t2s欧几里得 西元前三世纪的希腊数学家# 简体转繁体$ echo &apos;欧几里得 西元前三世纪的希腊数学家&apos; | opencc -c s2t歐幾里得 西元前三世紀的希臘數學家# 可以通过以下方式直接对文件进行繁简转换$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json","categories":[{"name":"辅助工具","slug":"辅助工具","permalink":"http://yoursite.com/categories/辅助工具/"}],"tags":[{"name":"opencc","slug":"opencc","permalink":"http://yoursite.com/tags/opencc/"},{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"}]},{"title":"pyltp - 哈工大语言云python接口使用说明","slug":"03-hitltp-tools","date":"2017-07-10T07:30:16.000Z","updated":"2017-07-13T16:04:08.234Z","comments":true,"path":"2017/07/10/03-hitltp-tools/","link":"","permalink":"http://yoursite.com/2017/07/10/03-hitltp-tools/","excerpt":"pyltp安装及模型下载可以使用pip直接安装，如果安装失败，建议下载源码进行手动编译。1pip install pyltp 安装pyltp后，下载模型文件，百度云地址在这。我下载的是ltp-data-v3.3.1.tar.bz2。然后将下载到的模型解压，存放在任意地方。注意：版本对应 pyltp版本：0.1.9 LTP版本：3.3.2 模型版本：3.3.1","text":"pyltp安装及模型下载可以使用pip直接安装，如果安装失败，建议下载源码进行手动编译。1pip install pyltp 安装pyltp后，下载模型文件，百度云地址在这。我下载的是ltp-data-v3.3.1.tar.bz2。然后将下载到的模型解压，存放在任意地方。注意：版本对应 pyltp版本：0.1.9 LTP版本：3.3.2 模型版本：3.3.1 上面两步都完成后，我们就可以使用pyltp进行一些文本操作了，例如：分句，分词，词性标注，命名实体识别以及依存句法等。 pyltp语言云的使用分句 - SentenceSplitter123from pyltp import SentenceSplittersentence = SentenceSplitter.split(&apos;我是逗号，我是句号。我是问号？我是感叹号！&apos;)print &apos;\\n&apos;.join(sentence) 分句结果如下：123我是逗号，我是句号。我是问号？我是感叹号！ 分词 - Segmentor12345678910import osLTP_DATA_DIR = &apos;/path/to/your/ltp_data&apos; # ltp模型目录的路径cws_model_path = os.path.join(LTP_DATA_DIR, &apos;cws.model&apos;) # 分词模型路径，模型名称为`cws.model`from pyltp import Segmentorsegmentor = Segmentor() # 初始化实例segmentor.load(cws_model_path) # 加载模型words = segmentor.segment(&apos;欧几里得是西元前三世纪的希腊数学家。&apos;) # 分词print &apos; &apos;.join(words)segmentor.release() # 释放模型 分词结果如下，【欧几里得】被拆成了四个单独的字。1欧 几 里 得 是 西元前 三 世纪 的 希腊 数学家 。 pyltp分词支持用户使用自定义词典。分词外部词典本身是一个文本文件，每行指定一个词，编码须为 UTF-8，样例如下所示:12欧几里得亚里士多德 使用自定义词典进行分词的模型加载方式如下：12345segmentor = Segmentor() # 初始化实例segmentor.load_with_lexicon(cws_model_path, &apos;/path/to/your/lexicon&apos;) # 加载模型，参数lexicon是自定义词典的文件路径words = segmentor.segment(&apos;欧几里得是西元前三世纪的希腊数学家。&apos;)print &apos; &apos;.join(words)segmentor.release() 自定义词典，分词结果如下，分词效果明显得到改善。1欧几里得 是 西元前 三 世纪 的 希腊 数学家 。 词性标注 - Postagger1234567891011pos_model_path = os.path.join(LTP_DATA_DIR, &apos;pos.model&apos;) # 词性标注模型路径，模型名称为`pos.model`from pyltp import Postaggerpostagger = Postagger() # 初始化实例postagger.load(pos_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = postagger.postag(words) # 词性标注print &apos; &apos;.join(postags)postagger.release() # 释放模型 词性标注结果如下，如果想了解更多的词性含义。请参考语言云词性标注简介。12345678910nh v nt m n u ns n wp# 欧几里得 - nh - 人名# 是 - v - 动词# 西元前 - nt - 时间名词# 三 - m - 数字# 世纪 - n - 普通名词# 的 - u - 助词# 希腊 - ns - 地理名词# 数学家- n - 普通名词# 。 - wp - 标点符号 命名实体识别 - NamedEntityRecognizer123456789101112ner_model_path = os.path.join(LTP_DATA_DIR, &apos;ner.model&apos;) # 命名实体识别模型路径，模型名称为`ner.model`from pyltp import NamedEntityRecognizerrecognizer = NamedEntityRecognizer() # 初始化实例recognizer.load(ner_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = [&apos;nh&apos;, &apos;v&apos;, &apos;nt&apos;, &apos;m&apos;, &apos;n&apos;, &apos;u&apos;, &apos;ns&apos;, &apos;n&apos;, &apos;wp&apos;]nertags = recognizer.recognize(words, postags) # 命名实体识别print &apos; &apos;.join(nertags)recognizer.release() # 释放模型 命名实体结果如下，ltp命名实体类型为：人名（Nh），地名（NS），机构名（Ni）；ltp采用BIESO标注体系。B表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成实体。123S-Nh O O O O O S-Ns O O# 欧几里得 - S-Nh - 人名# 希腊 - S-Ns - 地名 依存句法分析 - Parser123456789101112131415161718par_model_path = os.path.join(LTP_DATA_DIR, &apos;parser.model&apos;) # 依存句法分析模型路径，模型名称为`parser.model`from pyltp import Parserparser = Parser() # 初始化实例parser.load(par_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = [&apos;nh&apos;, &apos;v&apos;, &apos;nt&apos;, &apos;m&apos;, &apos;n&apos;, &apos;u&apos;, &apos;ns&apos;, &apos;n&apos;, &apos;wp&apos;]arcs = parser.parse(words, postags) # 句法分析rely_id = [arc.head for arc in arcs] # 提取依存父节点idrelation = [arc.relation for arc in arcs] # 提取依存关系heads = [&apos;Root&apos; if id == 0 else words[id-1] for id in rely_id] # 匹配依存父节点词语for i in range(len(words)): print relation[i] + &apos;(&apos; + words[i] + &apos;, &apos; + heads[i] + &apos;)&apos;parser.release() # 释放模型 依存句法分析，输出结果如下，关于依存句法分析，详细参照语言云依存句法简介。123456789SBV(欧几里得, 是)HED(是, Root)ATT(西元前, 世纪)ATT(三, 世纪)ATT(世纪, 数学家)RAD(的, 世纪)ATT(希腊, 数学家)VOB(数学家, 是)WP(。, 是)","categories":[{"name":"辅助工具","slug":"辅助工具","permalink":"http://yoursite.com/categories/辅助工具/"}],"tags":[{"name":"ltp","slug":"ltp","permalink":"http://yoursite.com/tags/ltp/"},{"name":"语言云","slug":"语言云","permalink":"http://yoursite.com/tags/语言云/"}]}]}