{"meta":{"title":"League of Data","subtitle":null,"description":"HOW NOT TO BE WRONG ?","author":"Xiao蜗牛","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2017-07-13T15:07:37.000Z","updated":"2017-08-26T12:29:44.779Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"123456789101112131415161718&#123; name: &apos;Xiao蜗牛&apos; age: 23, gender: &apos;男&apos;, address: &apos;广东省广州市&apos;, education: &apos;本科/延边大学&apos;, Github: &apos;https://github.com/guuboi&apos;, email: &apos;guuboi@163.com&apos;, skills: [ [&apos;Excel&apos;, &apos;Spss&apos;, &apos;R语言&apos;, &apos;Python&apos;], [&apos;MySql&apos;, &apos;Oracle&apos;, &apos;Hive&apos;, &apos;Pig&apos;], [&apos;数据分析&apos;, &apos;自然语言处理&apos;], [&apos;Linux&apos;] ], description: &apos;喜欢新事物，关注nlp前沿动态，对新的技术有追求； 喜欢研究，喜欢分析，喜欢 coding。&apos;&#125;"},{"title":"标签","date":"2017-07-13T15:05:02.000Z","updated":"2017-07-13T15:12:45.481Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-07-13T15:07:18.000Z","updated":"2017-07-13T15:12:14.966Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"CS224N - 自然语言处理 - word2vec","slug":"15-cs224n_note1","date":"2017-09-10T07:30:16.000Z","updated":"2017-09-11T16:09:25.469Z","comments":true,"path":"2017/09/10/15-cs224n_note1/","link":"","permalink":"http://yoursite.com/2017/09/10/15-cs224n_note1/","excerpt":"Problem 1: Softmax1.1 (a) 数值稳定性Prove: softmax的数值稳定性，即对任意的输入向量$x$和任意的常数$c$，softmax($x$) = softmax($x + c$)，$x + c$代表对向量$x$的每一个元素都加上常数$c$。其中， $$\\begin{equation} softmax(x)_{i} = \\frac{e^{x_{i}}}{\\sum_{j} e^{x_{j}}} \\end{equation}$$","text":"Problem 1: Softmax1.1 (a) 数值稳定性Prove: softmax的数值稳定性，即对任意的输入向量$x$和任意的常数$c$，softmax($x$) = softmax($x + c$)，$x + c$代表对向量$x$的每一个元素都加上常数$c$。其中， $$\\begin{equation} softmax(x)_{i} = \\frac{e^{x_{i}}}{\\sum_{j} e^{x_{j}}} \\end{equation}$$ Answer:我们可以通过提取因子$c$，然后进行消除进行证明，如下： $$\\begin{align} softmax(x + c)_{i} &amp;= \\frac{e^{x_{i} + c}}{\\sum_{j} e^{x_{j} + c}} % = \\frac{e^{x_{i}} \\times e^{c}}{e^{c} \\times \\sum_{j} e^{x_{j}}} \\nonumber \\\\ % &amp;= softmax(x)_{i} \\nonumber \\end{align}$$ 1.2 (b) python程序实现Softmax给定一个$M$行$N$列的输入矩阵，利用Sonftmax的数值稳定性计算每行的softmax: 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npdef softmax(x): &quot;&quot;&quot;Compute the softmax function for each row of the input x. You should also make sure that your code works for a single N-dimensional vector (treat the vector as a single row) and for M x N matrices. This may be useful for testing later. Also, make sure that the dimensions of the output match the input. You must implement the optimization in problem 1(a) of the written assignment! Arguments: x -- A N dimensional vector or M x N dimensional numpy matrix. Return: x -- You are allowed to modify x in-place &quot;&quot;&quot; orig_shape = x.shape if len(x.shape) &gt; 1: # Matrix exp_minmax = lambda x: np.exp(x - np.max(x)) denom = lambda x: 1.0 / np.sum(x) x = np.apply_along_axis(exp_minmax, 1, x) denominator = np.apply_along_axis(denom, 1, x) if len(denominator.shape) == 1: denominator = denominator.reshape((denominator.shape[0], 1)) x = x * denominator else: # Vector x_max = np.max(x) x = x - x_max numerator = np.exp(x) denominator = 1.0 / np.sum(numerator) x = numerator.dot(denominator) assert x.shape == orig_shape return x Problem 2: 神经网络基础知识2.1 (a) Sigmoid Gradient (Sigmoid梯度)假定输入x为标量，推导sigmoid函数的梯度，并证明其梯度函数能够用sigmoid函数本身表示。其中，sigmoid函数为：$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$ Answer:$$\\begin{align} \\sigma(x) &amp;= \\frac{1}{1 + e^{-x}} \\nonumber \\\\ % &amp;= \\frac{e^{x}}{1 + e^{x}} \\nonumber \\\\ % \\frac{\\partial}{\\partial x} \\sigma(x) &amp;= \\frac{e^{x} \\times (1 + e^{x}) - (e^{x} \\times e^{x})}{(1 + e^{x})^{2}} \\nonumber \\\\ % &amp;= \\frac{e^{x} + (e^{x} \\times e^{x}) - (e^{x} \\times e^{x})}{(1 + e^{x})^{2}} \\nonumber \\\\ % &amp;= \\frac{e^{x}}{(1 + e^{x})^{2}} = \\sigma(x) \\times (1 - \\sigma(x)) \\nonumber \\end{align}$$ 2.2 (b) 交叉熵损失函数推导交叉熵损失函数$CE(\\mathbf{y},\\hat{\\mathbf{y}}) = - \\sum_{i} y_{i} \\times log(\\hat{y_{i}})$的梯度。 其中，$\\hat{\\mathbf{y}} = softmax(\\mathbf{\\theta})$，为所有分类类别的预测概率向量，$\\mathbf{y}$为one-hot标签向量。 Answer:取$S$代表Softmax函数：$$\\begin{align} f_{i} &amp;= e^{\\theta_{i}} \\nonumber \\\\ % g_{i} &amp;= \\sum_{k=1}^{K} e^{\\theta_{k}} \\nonumber \\\\ % S_{i} &amp;= \\frac{f_{i}}{g_{i}} \\nonumber \\\\ % \\frac{\\partial S_{i}}{\\partial \\theta_{j}} &amp;= \\frac{f&apos;_{i} g_{i} - g&apos;_{i} f_{i}}{g_{i}^{2}} \\nonumber \\end{align}$$ 当$i$ = $j$时：$$\\begin{align} f&apos;_{i} &amp;= f_{i}; g&apos;_{i} = e^{\\theta_{j}} \\nonumber \\\\ % \\frac{\\partial S_{i}}{\\partial \\theta_{j}} % &amp;= \\frac{e^{\\theta_{i}} \\sum_{k} e^{\\theta_{k}} - e^{\\theta_{j}} e^{\\theta_{i}}}{(\\sum_{k} e^{\\theta_{k}})^{2}} \\nonumber \\\\ % &amp;= \\frac{e^{\\theta_{i}}}{\\sum_{k} e^{\\theta_{k}}} \\times \\frac{\\sum_{k} e^{\\theta_{k}} - e^{\\theta_{j}}}{\\sum_{k} e^{\\theta_{k}}} \\nonumber \\\\ % &amp;= S_{i} \\times (1 - S_{i}) \\nonumber \\end{align}$$ 当 $i \\ne j$时：$$\\begin{align} \\frac{\\partial S_{i}}{\\partial \\theta_{j}} % &amp;= \\frac{0 - e^{\\theta_{j}} e^{\\theta_{i}}}{(\\sum_{k} e^{\\theta_{k}})^{2}} \\nonumber \\\\ % &amp;= - \\frac{e^{\\theta_{j}}}{\\sum_{k} e^{\\theta_{k}}} \\times \\frac{e^{\\theta_{i}}}{\\sum_{k} e^{\\theta_{k}}} \\nonumber \\\\ % &amp;= -S_{j} \\times S_{i} \\nonumber \\end{align}$$ 用$L$代表交叉损失函数，则：$$\\begin{align} \\frac{\\partial L}{\\partial \\theta_{i}} % &amp;= - \\sum_{k} y_{k} \\frac{\\partial log S_{k}}{\\partial \\theta_{i}} \\nonumber \\\\ % &amp;= - \\sum_{k} y_{k} \\frac{1}{S_{k}} \\frac{\\partial S_{k}}{\\partial \\theta_{i}} \\nonumber \\\\ % &amp;= - y_{i} (1 - S_{i}) - \\sum_{k \\ne i} y_{k} \\frac{1}{S_{k}} (-S_{k} \\times S_{i}) \\nonumber \\\\ % &amp;= - y_{i} (1 - S_{i}) + \\sum_{k \\ne i} y_{k} S_{i} \\nonumber \\\\ % &amp;= - y_{i} + y_{i} S_{i} + \\sum_{k \\ne i} y_{k} S_{i} \\nonumber \\\\ % &amp;= S_{i}(\\sum_{k} y_{k}) - y_{i} \\nonumber \\end{align}$$ 因为$\\sum_{k} y_{k} = 1$，所以：$$\\begin{align} \\frac{\\partial L}{\\partial \\theta_{i}} % &amp;= S_{i} - y_{i} \\nonumber \\end{align}$$ 2.3 (c) 单个隐藏层的梯度 (One Hidden Layer Gradient)推导输入为$\\mathbf{x}$，隐藏层的激活函数为sigmoid，输出层为softmax的单层圣经网络的梯度。假定one-hot标签向量为$\\mathbf{y}$，损失函数为交叉熵。其中，前向传播如下：$$\\begin{align} \\mathbf{h} &amp;= sigmoid(\\mathbf{xW}_{1} + \\mathbf{b}_{1}) &amp; \\hat{\\mathbf{y}} = softmax(\\mathbf{hW}_{2} + \\mathbf{b}_{2}) \\nonumber \\end{align}$$ Answer:令$f_{2} = \\mathbf{xW}_{1} + \\mathbf{b}_{1}$，$f_{3} = \\mathbf{hW}_{2} + \\mathbf{b}_{2}$; $$\\begin{align} \\frac{\\partial J}{\\partial f_{3}} % &amp;= \\mathbf{\\delta}_{3} = \\hat{\\mathbf{y}} - \\mathbf{y} \\nonumber \\\\ % \\frac{\\partial J}{\\partial \\mathbf{h}} % &amp;= \\mathbf{\\delta}_{2} = \\mathbf{\\delta}_{3}\\mathbf{W}_{2}^{T} \\nonumber \\\\ % \\frac{\\partial J}{\\partial f_{2}} % &amp;= \\mathbf{\\delta}_{1} = \\mathbf{\\delta}_{2} \\circ \\sigma&apos;(f_{2}) \\nonumber \\\\ % \\frac{\\partial J}{\\partial \\mathbf{x}} % &amp;= \\mathbf{\\delta}_{1} \\frac{\\partial f_{2}}{\\partial \\mathbf{x}} \\nonumber \\\\ % &amp;= \\mathbf{\\delta}_{1} \\mathbf{W}_{1}^{T} \\nonumber \\end{align}$$ 2.4 (d) 参数个数(Number of Parameters)在上面(c)中的神经网络，有多少个参数？假定输入为$D_{x}$维，输出为$D_{y}$维，隐藏层共有$H$个神经单元。 Answer:$$\\begin{align} n_{W_{1}} &amp;= D_{x} \\times H \\nonumber \\\\ % n_{b_{1}} &amp;= H \\nonumber \\\\ % n_{W_{2}} &amp;= H \\times D_{y} \\nonumber \\\\ % n_{b_{2}} &amp;= D_{y} \\nonumber \\\\ % N &amp;= (D_{x} \\times H) + H + (H \\times D_{y}) + D_{y} \\nonumber \\end{align}$$ 2.5 (e) pyhton代码实现Sigmoid1234567891011121314151617181920212223242526272829303132import numpy as npdef sigmoid(x): &quot;&quot;&quot; Compute the sigmoid function for the input here. Arguments: x -- A scalar or numpy array. Return: s -- sigmoid(x) &quot;&quot;&quot; s = 1.0 / (1 + np.exp(-x)) return sdef sigmoid_grad(s): &quot;&quot;&quot; Compute the gradient for the sigmoid function here. Note that for this implementation, the input s should be the sigmoid function value of your original input x. Arguments: s -- A scalar or numpy array. Return: ds -- Your computed gradient. &quot;&quot;&quot; ds = s * (1 - s) return ds 2.6 (f) Python实现梯度检验(Gradient Check)为了方便debug，我们需要实现一个检验梯度的代码, 梯度检验的思想为梯度的极限定义，如下图所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npimport random# First implement a gradient checker by filling in the following functionsdef gradcheck_naive(f, x): &quot;&quot;&quot; Gradient check for a function f. Arguments: f -- a function that takes a single argument and outputs the cost and its gradients x -- the point (numpy array) to check the gradient at &quot;&quot;&quot; rndstate = random.getstate() random.setstate(rndstate) fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes in x it = np.nditer(x, flags=[&apos;multi_index&apos;], op_flags=[&apos;readwrite&apos;]) while not it.finished: ix = it.multi_index # Try modifying x[ix] with h defined above to compute # numerical gradients. Make sure you call random.setstate(rndstate) # before calling f(x) each time. This will make it possible # to test cost functions with built in randomness later. x[ix] += h random.setstate(rndstate) new_f1 = f(x)[0] x[ix] -= 2*h random.setstate(rndstate) new_f2 = f(x)[0] x[ix] += h numgrad = (new_f1 - new_f2) / (2 * h) # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print(&quot;Gradient check failed.&quot;) print(&quot;First gradient error found at index %s&quot; % str(ix)) print(&quot;Your gradient: %f \\t Numerical gradient: %f&quot; % ( grad[ix], numgrad)) return it.iternext() # Step to next dimension print(&quot;Gradient check passed!&quot;) 2.7 (g) python实现神经网络实现一个由sigmoid隐藏组成的层神经网络的前向以及后向传播，并检验代码的可行性。神经网络的网络结构如下： Answer:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# softmax, sigmoid, gradcheck_naive defined above!def forward_backward_prop(data, labels, params, dimensions): &quot;&quot;&quot; Forward and backward propagation for a two-layer sigmoidal network Compute the forward propagation for the cross entropy cost, and backward propagation for the gradients for all parameters. Arguments: data -- M x Dx matrix, where each row is a training example. labels -- M x Dy matrix, where each row is a one-hot vector. params -- Model parameters, these are unpacked for you. dimensions -- A tuple of input dimension, number of hidden units and output dimension &quot;&quot;&quot; ### Unpack network parameters (do not modify) ofs = 0 Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2]) W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) ofs += Dx * H b1 = np.reshape(params[ofs:ofs + H], (1, H)) ofs += H W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) ofs += H * Dy b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy)) ### forward propagation h = sigmoid(np.dot(data, W1) + b1) yhat = softmax(np.dot(h, W2) + b2) ### backward propagation cost = np.sum(-np.log(yhat[labels==1])) / data.shape[0] d3 = (yhat - labels) / data.shape[0] gradW2 = np.dot(h.T, d3) gradb2 = np.sum(d3, 0, keepdims=True) dh = np.dot(d3, W2.T) grad_h = sigmoid_grad(h) * dh gradW1 = np.dot(data.T,grad_h) gradb1 = np.sum(grad_h,0) ### END YOUR CODE ### Stack gradients (do not modify) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/categories/自然语言处理/"}],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"cs224n","slug":"cs224n","permalink":"http://yoursite.com/tags/cs224n/"}]},{"title":"Pearson相关系数Vs.Spearman相关系数","slug":"14-pearson_vs_spearman","date":"2017-08-24T07:30:16.000Z","updated":"2017-08-26T12:28:19.714Z","comments":true,"path":"2017/08/24/14-pearson_vs_spearman/","link":"","permalink":"http://yoursite.com/2017/08/24/14-pearson_vs_spearman/","excerpt":"统计术语中，相关系数一词经常被滥用，同时也困扰着我。相关系数描述一个变量随着另一个变量的增加而增加，也可以理解为单调递增。变量之间的这个单调趋势很值得去探索，但是大多数人习惯使用标准相关系数导致无法发现这一趋势。在我的印象中，老师在课堂上经常强调：我们现在所说的、以及以后所说的相关都指线性相关。所以，每当我们一提到相关性或者探寻变量间的相关性时，脑海里便跳出了线性相关。把变量间的相关性限制成了线性相关。","text":"统计术语中，相关系数一词经常被滥用，同时也困扰着我。相关系数描述一个变量随着另一个变量的增加而增加，也可以理解为单调递增。变量之间的这个单调趋势很值得去探索，但是大多数人习惯使用标准相关系数导致无法发现这一趋势。在我的印象中，老师在课堂上经常强调：我们现在所说的、以及以后所说的相关都指线性相关。所以，每当我们一提到相关性或者探寻变量间的相关性时，脑海里便跳出了线性相关。把变量间的相关性限制成了线性相关。 Pearson相关系数，通常是学生们学到的计算相关系数的唯一方法，此方法倾向于研究线性趋势。只有Spearman相关系数，实际上用于检测一般单调趋势，而这种方法通常在课堂上老师没有讲解。通过下面这幅图，你可以清晰地看到随着x的多项式次数的增加，Pearson和Spearman相关系数是如何变化的。 如果Pearson相关系数确实检测到了单调趋势，那么随着x多项式次数的增加，Pearson相关系数会向0靠拢，但不会为0。此时，在非线性相关的时候使用Spearman相关系数会更加精确。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/数据分析/"}],"tags":[{"name":"Pearson","slug":"Pearson","permalink":"http://yoursite.com/tags/Pearson/"},{"name":"Spearman","slug":"Spearman","permalink":"http://yoursite.com/tags/Spearman/"},{"name":"相关系数","slug":"相关系数","permalink":"http://yoursite.com/tags/相关系数/"}]},{"title":"基于gensim的Wiki百科中文word2vec训练","slug":"11-word2vec-based-on-gensim","date":"2017-07-13T07:30:16.000Z","updated":"2017-07-17T15:30:21.487Z","comments":true,"path":"2017/07/13/11-word2vec-based-on-gensim/","link":"","permalink":"http://yoursite.com/2017/07/13/11-word2vec-based-on-gensim/","excerpt":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。","text":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。 我们怎么得到一个词的word2vec呢？下面我们将介绍如何使用python gensim得到我们想要的词向量。总的来说，包括以下几个步骤： wiki中文数据预处理 文本数据分词 gensim word2vec训练 wiki中文数据预处理首先，下载wiki中文数据：zhwiki-latest-pages-articles.xml.bz2。因为zhwiki数据中包含很多繁体字，所以我们想获得简体语料库，接下来需要做以下两件事： 使用gensim模块中的WikiCorpus从bz2中获取原始文本数据 使用OpenCC将繁体字转换为简体字 WikiCorpus获取原始文本数据数据处理的python代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from __future__ import print_functionfrom gensim.corpora import WikiCorpusimport jiebaimport codecsimport osimport sixfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport multiprocessing class Config: data_path = &apos;xxx/zhwiki&apos; zhwiki_bz2 = &apos;zhwiki-latest-pages-articles.xml.bz2&apos; zhwiki_raw = &apos;zhwiki_raw.txt&apos; zhwiki_raw_t2s = &apos;zhwiki_raw_t2s.txt&apos; zhwiki_seg_t2s = &apos;zhwiki_seg.txt&apos; embedded_model_t2s = &apos;embedding_model_t2s/zhwiki_embedding_t2s.model&apos; embedded_vector_t2s = &apos;embedding_model_t2s/vector_t2s&apos; def dataprocess(_config): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) wiki = WikiCorpus(os.path.join(_config.data_path, _config.zhwiki_bz2), lemmatize=False, dictionary=&#123;&#125;) for text in wiki.get_texts(): if six.PY3: output.write(b&apos; &apos;.join(text).decode(&apos;utf-8&apos;, &apos;ignore&apos;) + &apos;\\n&apos;) else: output.write(&apos; &apos;.join(text) + &apos;\\n&apos;) i += 1 if i % 10000 == 0: print(&apos;Saved &apos; + str(i) + &apos; articles&apos;) output.close() print(&apos;Finished Saved &apos; + str(i) + &apos; articles&apos;)config = Config()dataprocess(config) 使用OpenCC将繁体字转换为简体字这里，需要预先安装OpenCC，关于OpenCC在linux环境中的安装方法，请参考这篇文章。仅仅需要两行linux命令就可以完成繁体字转换为简体字的认为，而且速度很快。12$ cd /xxx/zhwiki/$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json 文本数据分词对于分词这个任务，我们直接使用了python的jieba分词模块。你也可以使用哈工大的ltp或者斯坦福的nltk python接口进行分词，准确率及权威度挺高的。不过这两个安装的时候会花费很长时间，尤其是斯坦福的。关于jieba的分词处理代码，参考如下：1234567891011121314151617181920212223242526272829303132def is_alpha(tok): try: return tok.encode(&apos;ascii&apos;).isalpha() except UnicodeEncodeError: return Falsedef zhwiki_segment(_config, remove_alpha=True): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) print(&apos;Start...&apos;) with codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw_t2s), &apos;r&apos;, encoding=&apos;utf-8&apos;) as raw_input: for line in raw_input.readlines(): line = line.strip() i += 1 print(&apos;line &apos; + str(i)) text = line.split() if True: text = [w for w in text if not is_alpha(w)] word_cut_seed = [jieba.cut(t) for t in text] tmp = &apos;&apos; for sent in word_cut_seed: for tok in sent: tmp += tok + &apos; &apos; tmp = tmp.strip() if tmp: output.write(tmp + &apos;\\n&apos;) output.close()zhwiki_segment(config) gensim word2vec训练python的gensim模块提供了word2vec训练，为我们模型的训练提供了很大的方便。关于gensim的使用方法，可以参考基于Gensim的Word2Vec实践。本次训练的词向量大小size为50，训练窗口为5，最小词频为5，并使用了多线程，具体代码如下：123456789101112131415161718192021def word2vec(_config, saved=False): print(&apos;Start...&apos;) model = Word2Vec(LineSentence(os.path.join(_config.data_path, _config.zhwiki_seg_t2s)), size=50, window=5, min_count=5, workers=multiprocessing.cpu_count()) if saved: model.save(os.path.join(_config.data_path, _config.embedded_model_t2s)) model.save_word2vec_format(os.path.join(_config.data_path, _config.embedded_vector_t2s), binary=False) print(&quot;Finished!&quot;) return model def wordsimilarity(word, model): semi = &apos;&apos; try: semi = model.most_similar(word, topn=10) except KeyError: print(&apos;The word not in vocabulary!&apos;) for term in semi: print(&apos;%s,%s&apos; % (term[0],term[1]))model = word2vec(config, saved=True) word2vec训练已经完成，我们得到了想要的模型以及词向量，并保存到本地。下面我们分别查看同【宝马】和【腾讯】最相近的前10个词语。可以发现：和【宝马】相近的词大都属于汽车行业，而且是汽车品牌；和【腾讯】相近的词大都属于互联网行业。1234567891011121314151617181920212223&gt;&gt;&gt; wordsimilarity(word=u&apos;宝马&apos;, model=model)保时捷,0.92567974329固特异,0.888278841972劳斯莱斯,0.884045600891奥迪,0.881808757782马自达,0.881799697876亚菲特,0.880708634853欧宝,0.877104878426雪铁龙,0.876984715462玛莎拉蒂,0.868475496769桑塔纳,0.865387916565&gt;&gt;&gt; wordsimilarity(word=u&apos;腾讯&apos;, model=model)网易,0.880213916302优酷,0.873666107655腾讯网,0.87026232481广州日报,0.859486758709微信,0.835543811321天涯社区,0.834927380085李彦宏,0.832848489285土豆网,0.831390202045团购,0.829696238041搜狐网,0.825544642448 附相关数据及代码，包含：简体字转换后文本，分词后文本，以及50维word2vec词向量。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/categories/自然语言处理/"}],"tags":[{"name":"gensim","slug":"gensim","permalink":"http://yoursite.com/tags/gensim/"},{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"}]},{"title":"OpenCC - 简体繁体转换","slug":"12-opencc-tools","date":"2017-07-12T07:30:16.000Z","updated":"2017-07-17T15:30:32.158Z","comments":true,"path":"2017/07/12/12-opencc-tools/","link":"","permalink":"http://yoursite.com/2017/07/12/12-opencc-tools/","excerpt":"最近使用中文维基百科数据训练Word2Vec时，发现数据里面包含了很多繁体字，这就很尴尬了。这时候就知道OpenCC的强大了。哈哈，本来打算直接使用python里面的opencc模块的，但是在安装，编译opencc时遇到了各种错误。花费了很长时间，终于安装成功，但是文本处理起来效率很低。最终选择了直接在linux下安装OpenCC，处理的效率着实安慰了我受伤的心 – 很快，特别快。","text":"最近使用中文维基百科数据训练Word2Vec时，发现数据里面包含了很多繁体字，这就很尴尬了。这时候就知道OpenCC的强大了。哈哈，本来打算直接使用python里面的opencc模块的，但是在安装，编译opencc时遇到了各种错误。花费了很长时间，终于安装成功，但是文本处理起来效率很低。最终选择了直接在linux下安装OpenCC，处理的效率着实安慰了我受伤的心 – 很快，特别快。 好啦，接下来总结下OpenCC的安装方法，万一以后又用到它了呢？主要参考这篇博客： 检查下linux环境下是否已经安装cmake以及git，如果没有，那就通过yum安装好。 12$ yum install cmake$ yum install git 克隆下OpennCC开源项目OpennCC开源项目。 1$ git clone https://github.com/BYVoid/OpenCC 编译OpenCC 123$ cd OpenCC$ make$ make install 创建libopencc.so.2链接 如果不知道libopencc.so.2的路径，可以通过find / -name libopencc.so.2查找。1$ ln -s /usr/lib/libopencc.so.2 /usr/lib64/libopencc.so.2 通过查看 OpenCC 版本，检查OpenCC是否已经安装成功 1$ opencc --version 测试用例 12345678# 繁体转简体$ echo &apos;歐幾里得 西元前三世紀的希臘數學家&apos; | opencc -c t2s欧几里得 西元前三世纪的希腊数学家# 简体转繁体$ echo &apos;欧几里得 西元前三世纪的希腊数学家&apos; | opencc -c s2t歐幾里得 西元前三世紀的希臘數學家# 可以通过以下方式直接对文件进行繁简转换$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json","categories":[{"name":"辅助工具","slug":"辅助工具","permalink":"http://yoursite.com/categories/辅助工具/"}],"tags":[{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"},{"name":"opencc","slug":"opencc","permalink":"http://yoursite.com/tags/opencc/"}]},{"title":"pyltp - 哈工大语言云python接口使用说明","slug":"13-hitltp-tools","date":"2017-07-10T07:30:16.000Z","updated":"2017-07-17T15:30:41.508Z","comments":true,"path":"2017/07/10/13-hitltp-tools/","link":"","permalink":"http://yoursite.com/2017/07/10/13-hitltp-tools/","excerpt":"pyltp安装及模型下载可以使用pip直接安装，如果安装失败，建议下载源码进行手动编译。1pip install pyltp 安装pyltp后，下载模型文件，百度云地址在这。我下载的是ltp-data-v3.3.1.tar.bz2。然后将下载到的模型解压，存放在任意地方。注意：版本对应 pyltp版本：0.1.9 LTP版本：3.3.2 模型版本：3.3.1","text":"pyltp安装及模型下载可以使用pip直接安装，如果安装失败，建议下载源码进行手动编译。1pip install pyltp 安装pyltp后，下载模型文件，百度云地址在这。我下载的是ltp-data-v3.3.1.tar.bz2。然后将下载到的模型解压，存放在任意地方。注意：版本对应 pyltp版本：0.1.9 LTP版本：3.3.2 模型版本：3.3.1 上面两步都完成后，我们就可以使用pyltp进行一些文本操作了，例如：分句，分词，词性标注，命名实体识别以及依存句法等。 pyltp语言云的使用分句 - SentenceSplitter123from pyltp import SentenceSplittersentence = SentenceSplitter.split(&apos;我是逗号，我是句号。我是问号？我是感叹号！&apos;)print &apos;\\n&apos;.join(sentence) 分句结果如下：123我是逗号，我是句号。我是问号？我是感叹号！ 分词 - Segmentor12345678910import osLTP_DATA_DIR = &apos;/path/to/your/ltp_data&apos; # ltp模型目录的路径cws_model_path = os.path.join(LTP_DATA_DIR, &apos;cws.model&apos;) # 分词模型路径，模型名称为`cws.model`from pyltp import Segmentorsegmentor = Segmentor() # 初始化实例segmentor.load(cws_model_path) # 加载模型words = segmentor.segment(&apos;欧几里得是西元前三世纪的希腊数学家。&apos;) # 分词print &apos; &apos;.join(words)segmentor.release() # 释放模型 分词结果如下，【欧几里得】被拆成了四个单独的字。1欧 几 里 得 是 西元前 三 世纪 的 希腊 数学家 。 pyltp分词支持用户使用自定义词典。分词外部词典本身是一个文本文件，每行指定一个词，编码须为 UTF-8，样例如下所示:12欧几里得亚里士多德 使用自定义词典进行分词的模型加载方式如下：12345segmentor = Segmentor() # 初始化实例segmentor.load_with_lexicon(cws_model_path, &apos;/path/to/your/lexicon&apos;) # 加载模型，参数lexicon是自定义词典的文件路径words = segmentor.segment(&apos;欧几里得是西元前三世纪的希腊数学家。&apos;)print &apos; &apos;.join(words)segmentor.release() 自定义词典，分词结果如下，分词效果明显得到改善。1欧几里得 是 西元前 三 世纪 的 希腊 数学家 。 词性标注 - Postagger1234567891011pos_model_path = os.path.join(LTP_DATA_DIR, &apos;pos.model&apos;) # 词性标注模型路径，模型名称为`pos.model`from pyltp import Postaggerpostagger = Postagger() # 初始化实例postagger.load(pos_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = postagger.postag(words) # 词性标注print &apos; &apos;.join(postags)postagger.release() # 释放模型 词性标注结果如下，如果想了解更多的词性含义。请参考语言云词性标注简介。12345678910nh v nt m n u ns n wp# 欧几里得 - nh - 人名# 是 - v - 动词# 西元前 - nt - 时间名词# 三 - m - 数字# 世纪 - n - 普通名词# 的 - u - 助词# 希腊 - ns - 地理名词# 数学家- n - 普通名词# 。 - wp - 标点符号 命名实体识别 - NamedEntityRecognizer123456789101112ner_model_path = os.path.join(LTP_DATA_DIR, &apos;ner.model&apos;) # 命名实体识别模型路径，模型名称为`ner.model`from pyltp import NamedEntityRecognizerrecognizer = NamedEntityRecognizer() # 初始化实例recognizer.load(ner_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = [&apos;nh&apos;, &apos;v&apos;, &apos;nt&apos;, &apos;m&apos;, &apos;n&apos;, &apos;u&apos;, &apos;ns&apos;, &apos;n&apos;, &apos;wp&apos;]nertags = recognizer.recognize(words, postags) # 命名实体识别print &apos; &apos;.join(nertags)recognizer.release() # 释放模型 命名实体结果如下，ltp命名实体类型为：人名（Nh），地名（NS），机构名（Ni）；ltp采用BIESO标注体系。B表示实体开始词，I表示实体中间词，E表示实体结束词，S表示单独成实体，O表示不构成实体。123S-Nh O O O O O S-Ns O O# 欧几里得 - S-Nh - 人名# 希腊 - S-Ns - 地名 依存句法分析 - Parser123456789101112131415161718par_model_path = os.path.join(LTP_DATA_DIR, &apos;parser.model&apos;) # 依存句法分析模型路径，模型名称为`parser.model`from pyltp import Parserparser = Parser() # 初始化实例parser.load(par_model_path) # 加载模型words = [&apos;欧几里得&apos;, &apos;是&apos;, &apos;西元前&apos;, &apos;三&apos;, &apos;世纪&apos;, &apos;的&apos;, &apos;希腊&apos;, &apos;数学家&apos;, &apos;。&apos;]postags = [&apos;nh&apos;, &apos;v&apos;, &apos;nt&apos;, &apos;m&apos;, &apos;n&apos;, &apos;u&apos;, &apos;ns&apos;, &apos;n&apos;, &apos;wp&apos;]arcs = parser.parse(words, postags) # 句法分析rely_id = [arc.head for arc in arcs] # 提取依存父节点idrelation = [arc.relation for arc in arcs] # 提取依存关系heads = [&apos;Root&apos; if id == 0 else words[id-1] for id in rely_id] # 匹配依存父节点词语for i in range(len(words)): print relation[i] + &apos;(&apos; + words[i] + &apos;, &apos; + heads[i] + &apos;)&apos;parser.release() # 释放模型 依存句法分析，输出结果如下，关于依存句法分析，详细参照语言云依存句法简介。123456789SBV(欧几里得, 是)HED(是, Root)ATT(西元前, 世纪)ATT(三, 世纪)ATT(世纪, 数学家)RAD(的, 世纪)ATT(希腊, 数学家)VOB(数学家, 是)WP(。, 是)","categories":[{"name":"辅助工具","slug":"辅助工具","permalink":"http://yoursite.com/categories/辅助工具/"}],"tags":[{"name":"ltp","slug":"ltp","permalink":"http://yoursite.com/tags/ltp/"},{"name":"语言云","slug":"语言云","permalink":"http://yoursite.com/tags/语言云/"}]},{"title":"Theano - 广播","slug":"09-Theano-08","date":"2017-05-08T07:30:16.000Z","updated":"2017-07-17T15:31:58.224Z","comments":true,"path":"2017/05/08/09-Theano-08/","link":"","permalink":"http://yoursite.com/2017/05/08/09-Theano-08/","excerpt":"广播（Broadcasting)广播是这样的一个机制：它允许不同维度的张量进行加法或者乘法运算。在运算时，他将会沿着维度缺失的方向复制较小的那个张量。 通过广播机制，一个标量可以被加到矩阵上，一个向量可以被加到矩阵上，或者一个标量可以被加到向量上。","text":"广播（Broadcasting)广播是这样的一个机制：它允许不同维度的张量进行加法或者乘法运算。在运算时，他将会沿着维度缺失的方向复制较小的那个张量。 通过广播机制，一个标量可以被加到矩阵上，一个向量可以被加到矩阵上，或者一个标量可以被加到向量上。 如上图，广播一个行矩阵。T和F分别表示True和False,指明沿着哪个维度可以进行广播。如果第二个参数是向量，它的形状为（2，）以及它的广播模式为（False,）。它将会自动向左展开，匹配矩阵的维度，最终得到（1,2）和（True,Fale）。 不像numpy那样动态地进行广播，Theano需要知道哪些维度需要进行广播。当可用的时候，广播信息将会以变量的类型给出。 下面的代码说明为了和矩阵执行加法运算，行和列怎么进行广播的：123456789101112131415161718192021222324252627282930313233343536import theanoimport numpyimport theano.tensor as Tr = T.row()r.broadcastable# (True, False)mtr = T.matrix()mtr.broadcastable# (False, False)f_row = theano.function([r, mtr], [r + mtr])R = numpy.arange(3).reshape(1,3)R# array([[0, 1, 2]])M = numpy.arange(9).reshape(3, 3)M# array([[0, 1, 2],# [3, 4, 5],# [6, 7, 8]])f_row(R, M)# [array([[ 0., 2., 4.],# [ 3., 5., 7.],# [ 6., 8., 10.]])]c = T.col()c.broadcastable# (False, True)f_col = theano.function([c, mtr], [c + mtr])C = numpy.arange(3).reshape(3, 1)C# array([[0],# [1],# [2]])M = numpy.arange(9).reshape(3, 3)f_col(C, M)# [array([[ 0., 1., 2.],# [ 4., 5., 6.],# [ 8., 9., 10.]])]","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 处理形状信息","slug":"08-Theano-07","date":"2017-05-07T07:30:16.000Z","updated":"2017-07-17T15:30:06.070Z","comments":true,"path":"2017/05/07/08-Theano-07/","link":"","permalink":"http://yoursite.com/2017/05/07/08-Theano-07/","excerpt":"Theano是怎么处理形状信息(Shape Information)在构建图的时候，不可能严格执行Theano变量的形状。因为在运行的时候，传递给Theano函数的某一参数的值可能影响Thenao变量的形状。目前，关于形状信息的使用在Theano中有以下两种方式： 在输出形状已知的情况下，生成在CPU和GPU上进行2d卷积的更高效的C代码 当我们只关心变量的形状，而不是实际值的时候，将移除图的计算。这通过Op.infer_shape完成。","text":"Theano是怎么处理形状信息(Shape Information)在构建图的时候，不可能严格执行Theano变量的形状。因为在运行的时候，传递给Theano函数的某一参数的值可能影响Thenao变量的形状。目前，关于形状信息的使用在Theano中有以下两种方式： 在输出形状已知的情况下，生成在CPU和GPU上进行2d卷积的更高效的C代码 当我们只关心变量的形状，而不是实际值的时候，将移除图的计算。这通过Op.infer_shape完成。 例子：12345678910import theanoimport theano.tensor as Tx = T.matrix(&apos;x&apos;)f = theano.function([x], (x ** 2).shape)theano.printing.debugprint(f)# MakeVector&#123;dtype=&apos;int64&apos;&#125; [id A] &apos;&apos; 2# |Shape_i&#123;0&#125; [id B] &apos;&apos; 1# | |x [id C]# |Shape_i&#123;1&#125; [id D] &apos;&apos; 0# |x [id C] 输出结果不包含任何乘法以及幂运算。Theano已经移除了它们直接去计算输出的形状。 形状推断问题（Shape Inference Problem）在图中，Theano将会传播形状的信息。有时，这将会导致一些错误。考虑下面的例子：123456789101112131415161718192021222324252627282930313233import numpyimport theanox = theano.tensor.matrix(&apos;x&apos;)y = theano.tensor.matrix(&apos;y&apos;)z = theano.tensor.join(0, x, y) # 将x,y按行拼接起来，要求x,y的列数一致xv = numpy.random.rand(5, 4)yv = numpy.random.rand(3, 3)f = theano.function([x, y], z.shape)theano.printing.debugprint(f)# MakeVector&#123;dtype=&apos;int64&apos;&#125; [id A] &apos;&apos; 4# |Elemwise&#123;Add&#125;[(0, 0)] [id B] &apos;&apos; 3# | |Shape_i&#123;0&#125; [id C] &apos;&apos; 1# | | |x [id D]# | |Shape_i&#123;0&#125; [id E] &apos;&apos; 2# | |y [id F]# |Shape_i&#123;1&#125; [id G] &apos;&apos; 0# |x [id D]f(xv, yv) # 并没有报错# array([8, 4])f = theano.function([x,y], z) # 直接返回ztheano.printing.debugprint(f)# Join [id A] &apos;&apos; 0# |TensorConstant&#123;0&#125; [id B]# |x [id C]# |y [id D]f(xv, yv) # 报错# Traceback (most recent call last):# ...# ValueError: ... 正如你看到的，当仅仅访问计算结果的形状信息(z.shape)时，将会直接推断结果的形状，并不会执行计算过程（即z的具体数值）。 这使得形状的计算速度很快，但是它可能会隐藏一些错误。在这个例子中，输出结果形状的计算仅仅基于输入的第一个Theano变量，这导致返回形状信息的错误。 这种现象也可能出现在其他运算上，比如elemwise和dot。事实上，为了执行一些优化（例如，速度和稳定性），Theano从一开始就假定计算是正确的，并且是一致的。就像上述例子中一样。你可以通过使用Theano标志optimizer_excluding=local_shape_to_shape_i运行代码（将不会执行上述提及的优化）来检测这种错误。你也可以通过在FAST_COMPILE或者DebugMode模式下执行代码，得到同样的效果。 FAST_COMPILE模式将不会执行这种优化，以及大部分其它的优化。 DebugMode模式将会在优化前以及优化后进行测试，导致运行速率更慢。 指定确切的形状目前，指定一个形状并不像我们计划一些更新和期望的那么容易和灵活。以下情形是目前我们可以做到的： 当调用conv2d时，你可以直接把形状信息传递给ConvOp。你只需要在调用时简单地设置一下image_shape和filter_shape参数就可以了。他们必须是包含4个元素的元组。例如： 1theano.tensor.nnet.conv2d(..., image_shape=(7,3,5,5), filter_shape=(2,3,4,4)) 你可以在图的任何位置使用SpecifyShape添加位置信息。这允许执行一些优化。在接下来的例子中，这使得预先计算Theano函数为常数成为可能。 12345import theanox = theano.tensor.matrix()x_specify_shape = theano.tensor.specify_shape(x, (2,2))f = theano.function([x], (x_specify_shape ** 2).shape)theano.printing.debugprint(f)","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 循环","slug":"07-Theano-06","date":"2017-05-06T07:30:16.000Z","updated":"2017-07-17T15:29:57.009Z","comments":true,"path":"2017/05/06/07-Theano-06/","link":"","permalink":"http://yoursite.com/2017/05/06/07-Theano-06/","excerpt":"Scan 复发(Recurrence)的一种常用形式，可以用于循环(looping) Reduction和map是scan的特例 可以根据一些输出序列scan一个函数（function)，每一步都会生成一个输出 可以查看之前k步的输出 给定一个初始状态z=0,可以通过scan函数z + x(i)计算一个列表的和sum(a_list) 通常一个for循环可以用scan()操作符进行实现 使用scan的优点： 迭代次数为符号图的一部分 最大限度地减少GPU传输（如果用到了GPU） 通过序列步长计算梯度 运行速率比python内置的for循环稍微快些 可以通过检测需要的实际内存量，来降低整体内存使用量","text":"Scan 复发(Recurrence)的一种常用形式，可以用于循环(looping) Reduction和map是scan的特例 可以根据一些输出序列scan一个函数（function)，每一步都会生成一个输出 可以查看之前k步的输出 给定一个初始状态z=0,可以通过scan函数z + x(i)计算一个列表的和sum(a_list) 通常一个for循环可以用scan()操作符进行实现 使用scan的优点： 迭代次数为符号图的一部分 最大限度地减少GPU传输（如果用到了GPU） 通过序列步长计算梯度 运行速率比python内置的for循环稍微快些 可以通过检测需要的实际内存量，来降低整体内存使用量 例子：对应元素计算tanh(x(t).dot(W) + b)123456789101112131415161718192021import theanoimport theano.tensor as Timport numpy as np# 定义张量变量X = T.matrix(&apos;X&apos;)W = T.matrix(&apos;W&apos;)b_sym = T.vector(&apos;b_sym&apos;)results, updates = theano.scan(lambda v: T.tanh(T.dot(v, W) + b_sym), sequences=X)compute_elementwise = theano.function([X, W, b_sym], results)# 测试x = np.eye(2, dtype=theano.config.floatX)w = np.ones((2, 2), dtype=theano.config.floatX)b = np.ones((2), dtype=theano.config.floatX)b[1] = 2compute_elementwise(x, w, b)# 和numpy相比较np.tanh(x.dot(w) + b) 例子： 计算序列x(t) = tanh(x(t-1).dot(W) + y(t).dot(U) + p(T-t).dot(V))123456789101112131415161718192021222324252627282930313233import theanoimport theano.tensor as Timport numpy as np# 定义张量变量X = T.vector(&apos;X&apos;)W = T.matrix(&apos;W&apos;)b_sym = T.vector(&apos;b_sym&apos;)U, Y, V, P = T.matrices(&apos;U&apos;, &apos;Y&apos;, &apos;V&apos;, &apos;P&apos;)result, update = theano.scan(lambda y, p, x_tml: T.tanh(T.dot(x_tml, W) + T.dot(y, U) + T.dot(p, V)), sequences=[Y, P[::-1]], outputs_info=[X])compute_seq = theano.function(inputs=[X, W, Y, U, P, V], outputs=result)# 测试x = np.zeros((2), dtype=theano.config.floatX)x[1] = 1w = np.ones((2, 2), dtype=theano.config.floatX)y = np.ones((5, 2), dtype=theano.config.floatX)y[0, :] = -3u = np.ones((2, 2), dtype=theano.config.floatX)p = np.ones((5, 2), dtype=theano.config.floatX)p[0, :] = 3v = np.ones((2, 2), dtype=theano.config.floatX)print(compute_seq(x, w, y, u, p, v))# 与Numpy对比x_res = np.zeros((5, 2), dtype=theano.config.floatX)x_res[0] = np.tanh(x.dot(w) + y[0].dot(u) + p[4].dot(v))for i in range(1, 5): x_res[i] = np.tanh(x_res[i - 1].dot(w) + y[i].dot(u) + p[4-i].dot(v))print(x_res) 例子： 计算X的行范式123456789101112131415import theanoimport theano.tensor as Timport numpy as np# 定义张量变量X = T.matrix(&apos;X&apos;)results, updates = theano.scan(lambda x_i: T.sqrt((x_i ** 2)).sum(), sequences=[X])compute_norm_lines = theano.function(inputs=[X], outputs=results)# 测试x = np.diag(np.arange(1, 6, dtype=theano.config.floatX), 1)print(compute_norm_lines(x))# 和Numpy对比print(np.sqrt((x ** 2).sum(1))) 例子： 计算X的列范式123456789101112131415import theanoimport theano.tensor as Timport numpy as np# 定义张量变量X = T.matrix(&quot;X&quot;)results, updates = theano.scan(lambda x_i: T.sqrt((x_i ** 2).sum()), sequences=[X.T])compute_norm_cols = theano.function(inputs=[X], outputs=results)# 测试x = np.diag(np.arange(1, 6, dtype=theano.config.floatX), 1)print(compute_norm_cols(x))# 和Numpy对比print(np.sqrt((x ** 2).sum(0)))","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 条件","slug":"06-Theano-05","date":"2017-05-05T07:30:16.000Z","updated":"2017-07-17T15:29:49.068Z","comments":true,"path":"2017/05/05/06-Theano-05/","link":"","permalink":"http://yoursite.com/2017/05/05/06-Theano-05/","excerpt":"IfElse vs Switch IfElse接收布尔型条件和两个变量作为输入。 Switch接收一个张量（Tensor）以及两个变量作为输入。 Switch进行元素级运算，因此比IfElse更常用。 IfElse比较懒惰，只计算满足条件的相应输出变量， 而Switch计算所有的输出变量。 即： ifelse(condition, output1, output2): 如果condition:1(0),那么ifelse只计算output1(output2)并输出。 switch(condition, output1, output2): 如果condition:1(0),那么switch计算output1和output2，并输出output1(output2)","text":"IfElse vs Switch IfElse接收布尔型条件和两个变量作为输入。 Switch接收一个张量（Tensor）以及两个变量作为输入。 Switch进行元素级运算，因此比IfElse更常用。 IfElse比较懒惰，只计算满足条件的相应输出变量， 而Switch计算所有的输出变量。 即： ifelse(condition, output1, output2): 如果condition:1(0),那么ifelse只计算output1(output2)并输出。 switch(condition, output1, output2): 如果condition:1(0),那么switch计算output1和output2，并输出output1(output2) 12345678910111213141516171819202122232425262728from theano import tensor as Tfrom theano.ifelse import ifelseimport theano, time, numpya, b = T.scalars(&apos;a&apos;, &apos;b&apos;)x, y = T.matrices(&apos;x&apos;, &apos;y&apos;)z_switch = T.switch(T.lt(a, b), T.mean(x), T.mean(y))z_ifelse = ifelse(T.lt(a, b), T.mean(x), T.mean(y))f_switch = theano.function([a, b, x, y], z_switch, mode=theano.Mode(linker=&apos;vm&apos;))f_ifelse = theano.function([a, b, x, y], z_ifelse, mode=theano.Mode(linker=&apos;vm&apos;))val1 = 0.val2 = 1.big_mat1 = numpy.ones((10000, 1000))big_mat2 = numpy.ones((10000, 1000))n_times = 10tic = time.clock()for i in range(n_times): f_switch(val1, val2, big_mat1, big_mat2)print(&apos;time spent evaluating both values %f sec&apos; % (time.clock() - tic))tic = time.clock()for j in range(n_times): f_ifelse(val1, val2, big_mat1, big_mat2)print(&apos;time spent evaluating one value %f sec&apos; % (time.clock() - tic)) 在这个例子中，IfElse比Switch花费更少的时间，因为他只计算输出变量中的一个。如果不使用linker=’vm’或linker=’cvm’，那么ifelse将会和switch一样计算两个输出变量，而且花费的时间和switch一样多。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 导数","slug":"05-Theano-04","date":"2017-05-04T07:30:16.000Z","updated":"2017-07-17T15:29:41.742Z","comments":true,"path":"2017/05/04/05-Theano-04/","link":"","permalink":"http://yoursite.com/2017/05/04/05-Theano-04/","excerpt":"计算梯度计算x^2的梯度123456789101112import numpyimport theanoimport theano.tensor as Tfrom theano import ppx = T.dscalar(&apos;x&apos;)y = x ** 2gy = T.grad(y, x)pp(gy)f = theano.function([x], gy)pp(f.maker.fgraph.outputs[0])f(4)numpy.allclose(f(94.2), 188.4) 计算逻辑函数的梯度12345x = T.dmatrix(&apos;x&apos;)s = T.sum(1 / (1 + T.exp(-x)))gs = T.grad(s, x)dlogistic = theano.function([x], gs)dlogistic([[0, 1], [-1, -2]])","text":"计算梯度计算x^2的梯度123456789101112import numpyimport theanoimport theano.tensor as Tfrom theano import ppx = T.dscalar(&apos;x&apos;)y = x ** 2gy = T.grad(y, x)pp(gy)f = theano.function([x], gy)pp(f.maker.fgraph.outputs[0])f(4)numpy.allclose(f(94.2), 188.4) 计算逻辑函数的梯度12345x = T.dmatrix(&apos;x&apos;)s = T.sum(1 / (1 + T.exp(-x)))gs = T.grad(s, x)dlogistic = theano.function([x], gs)dlogistic([[0, 1], [-1, -2]]) 计算Jacobian12345x = T.dvector(&apos;x&apos;)y = x ** 2J, updates = theano.scan(lambda i, y, x: T.grad(y[i], x), sequences=T.arange(y.shape[0]), non_sequences=[y,x])f = theano.function([x], J, updates=updates)f([4, 4]) 计算Hessian矩阵1234567x = T.dvector(&apos;x&apos;)y = x ** 2cost = y.sum()gy = T.grad(cost, x)H, updates = theano.scan(lambda i, gy, x: T.grad(gy[i], x), sequences=T.arange(gy.shape[0]), non_sequences=[gy,x])f = theano.function([x], H, updates=updates)f([4,4]) Jacobian times a Vector右算子(R-operator)1234567W = T.dmatrix(&apos;W&apos;)V = T.dmatrix(&apos;V&apos;)x = T.dvector(&apos;x&apos;)y = T.dot(x, W)JV = T.Rop(y, W, V)f = theano.function([W, V, x], JV)f([[1,1], [1,1]], [[2,2], [2,2]], [0,1]) 左算子（L-operator)1234567W = T.dmatrix(&apos;W&apos;)v = T.dvector(&apos;v&apos;)x = T.dvector(&apos;x&apos;)y = T.dot(x, W)VJ = T.Lop(y, W, v)f = theano.function([v, x], VJ)f([2,2], [0,1]) Hessian times a Vector1234567x = T.dvector(&apos;x&apos;)v = T.dvector(&apos;v&apos;)y = T.sum(x ** 2)gy = T.grad(y, x)vH = T.grad(T.sum(gy * v), x)f = theano.function([x,v], vH)f([4,4], [2,2]) 右算子1234567x = T.dvector(&apos;x&apos;)v = T.dvector(&apos;v&apos;)y = T.sum(x ** 2)gy = T.grad(y, x)Hv = T.Rop(gy, x, v)f = theano.function([x,v], Hv)f([4,4], [2,2])","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 更多的例子","slug":"04-Theano-03","date":"2017-05-03T07:30:16.000Z","updated":"2017-07-17T15:29:32.413Z","comments":true,"path":"2017/05/03/04-Theano-03/","link":"","permalink":"http://yoursite.com/2017/05/03/04-Theano-03/","excerpt":"Logistic函数12345678910import theanoimport theano.tensor as Tx = T.dmatrix(&apos;x&apos;)s = 1 / (1 + T.exp(-x))logistic = theano.function([x], s)logistic([[0, 1], [-1, -2]])# s(x) = 1/(1+exp(-x)) = (1+tanh(x/2))/2s2 = (1 + T.tanh(x / 2)) / 2logistic2 = theano.function([x], s2)logistic2([[0, 1], [-1, -2]]) 同时执行多种计算任务Theano支持多种输出的函数。例如，我们可以同时计算两个矩阵a,b相应元素之间的差、绝对差、平方差。当我们调用函数f是，返回三个变量：","text":"Logistic函数12345678910import theanoimport theano.tensor as Tx = T.dmatrix(&apos;x&apos;)s = 1 / (1 + T.exp(-x))logistic = theano.function([x], s)logistic([[0, 1], [-1, -2]])# s(x) = 1/(1+exp(-x)) = (1+tanh(x/2))/2s2 = (1 + T.tanh(x / 2)) / 2logistic2 = theano.function([x], s2)logistic2([[0, 1], [-1, -2]]) 同时执行多种计算任务Theano支持多种输出的函数。例如，我们可以同时计算两个矩阵a,b相应元素之间的差、绝对差、平方差。当我们调用函数f是，返回三个变量： 12345678import theanoimport theano.tensor as Ta, b = T.dmatrices(&apos;a&apos;, &apos;b&apos;)diff = a - babs_diff = abs(diff)diff_squared = diff ** 2f = theano.function([a, b], [diff, abs_diff, diff_squared])f([[1, 1], [1, 1]], [[0, 1], [2, 3]]) 为参数设置默认值假设我们要定义一个实现两个数字加法的函数。如果你仅仅提供一个数字，另一个数字假设(默认)为1,就可以这么做：1234567from theano import In, functionimport theano.tensor as Tx, y = T.dscalars(&apos;x&apos;, &apos;y&apos;)z = x + yf = function([x, In(y, value=1)], z)f(33)f(33, 2) 含有默认值的输入必须位于不含默认值的输入之后（和python的函数类似）。允许多个输入含有默认值，这些参数可以通过位置设定，也可以通过名字进行设定。12345678x, y, w = T.dscalars(&apos;x&apos;, &apos;y&apos;, &apos;w&apos;)z = (x + y) * wf = function([x, In(y, value=1), In(w, value=2, name=&apos;w_by_name&apos;)], z)f(33)f(33, 2)f(33, 0, 1)f(33, w_by_name=1)f(33, w_by_name=1, y=0) In 不知道通过参数传递的局部变量x,y的名字。符号变量对象拥有名字（name）属性（在上本例中通过dscalars进行设置），这也是我们构建函数function关键字参数的名字。通过In(y, value=1)这一机制实现。在In(w, value=2, name=’w_by_name’)中，我们重写了符号变量的名字属性。所有当我们通过f(x=33, y=0, w=1)的形式调用函数时，就会出错。w应该改为w_by_name. 使用共享变量我们也可以构建一个含有内状态（internal state）的函数。例如,假设我们要构造一个累加函数（accumulator）:初始状态设置为0。接着，每次调用函数，状态就会通过函数的参数自动增加。12345678910111213141516171819202122232425262728293031# 首先，我们定义一个累加函数。它将自己的内状态加上它的参数，然后返回旧状态的值。import theanoimport theano.tensor as Tfrom theano import sharedstate = shared(0)inc = T.iscalar(&apos;inc&apos;)accumulator = function([inc], state, updates=[(state, state+inc)])# state的值可以通过.get_value()和.set_value()惊行获取和修改state.get_value()accumulator(1)state.get_value()accumulator(300)state.get_value()state.set_value(-1)accumulator(3)state.get_value()# 我们可以构造多个函数，使用相同共享变量，这些函数都可以更新状态的值decrementor = function([inc], state, updates=[(state, state-inc)])decrementor(2)state.get_value()# 可能你会使用一个共享变量表达多个公式，但是你并不想使用共享变量的值。# 这种情况下，你可以使用function中的givens参数。fn_of_state = state * 2 + incfoo = T.scalar(dtype=state.dtype) # foo的类型必须和将要通过givens取代的共享变量的类型保持一致skip_shared = function([inc, foo], fn_of_state, givens=[(state, foo)])skip_shared(1, 3) # 我们正在使用3作为state,并非state.valuestate.get_value() # 旧的状态(state)一直存在，但是我们使用它。 复制函数（copying functions)Theano中的函数可以被复制，被用于构造相似的函数（拥有不同的共享变量和更新），这可以通过function中的copy()实现。让我们从以上定义的累加函数(accumulator)开始：123456789101112131415161718192021import theanoimport theano.tensor as Tstate = theano.shared(0)inc = T.iscalar(&apos;inc&apos;)accumulator = function([inc], state, updates=[(state, state+inc)])# 我们可以像平常一样增加它的状态（state）accumulator(10)state.get_value()# 我们可以用copy()创建一个相似的累加器(accumulator)，但是可以通过swap参数拥有自己的内状态,# swap参数是将要交换的共享参数字典new_state = theano.shared(0)new_accumulator = accumulator.copy(swap=&#123;state:new_state&#125;)new_accumulator(100)new_state.get_value()state.get_value()# 现在我们创建一个复制，但是使用delete_updates参数移除更新，此时，默认为False# 此时，共享状态将不会再更新。null_accumulator = accumulator.copy(delete_updates=True)null_accumulator(9000)state.get_value() 使用随机数（Using Random Numbers)简洁的例子123456789101112131415161718192021222324from theano.tensor.shared_randomstreams import RandomStreamsfrom theano import functionsrng = RandomStreams(seed=324)rv_u = srng.uniform((2,2))rv_n = srng.normal((2,2))f = function([], rv_u)g = function([], rv_n, no_default_updates=True) # 不更新rv_n.rngnearly_zeros = function([], rv_u + rv_u - 2 * rv_u)# rv_u表示服从均匀分布的2*2随机数矩阵# rv_n表示服从正太分布的2*2随机数矩阵# 现在我们来调用这些对象。如果调用f()，我们将会得到随机均匀分布数。# 随机数产生器的内状态将会自动更新，所以我们每次调用f()时将会得到不同的随机数f_val0 = f()f_val1 = f()# 当我们添加额外的参数no_default_updates=True（在函数g中）后，随机数产生器的状态将不会受调用函数的影响。# 例如：多次调用g()将会返回相同的随机数,g_val0和g_val1相同。g_val0 = g()g_val1 = g()# 一个重要的观点是：一个随机变量在一次调用函数期中最多只能构建一次。# 所以nearly_zeros函数保证了输出近似为0，尽管rv_u随机变量在输出表达式中出现了3次。nearly_zeros() 种子流（Seeding Streams）随机变量可以单独也可以共同产生，你可以通过对.rng属性进行seeding或者使用.rng.set_value()对.rng进行赋值产生一个随机变量。123456rng_val = rv_u.rng.get_value(borrow=True) # 获取rv_u的rng(随机数生成器)rng_val.seed(89234) # 对generator(生成器）进行seeds（播种）rv_u.rng.set_value(rng_val, borrow=True) # 对rng进行赋值# 你可以seed由RandomStreams对象分配的所有随机变量。srng.seed(902340) 函数之间共享流（Sharing Streams Between Functions）像共享变量一样，随机变量使用的随机数生成器在不同函数之间是相同的。所以我们的nearly_zeros函数将会更新f函数使用的生成器的状态。例如：12345678state_after_v0 = rv_u.rng.get_value().get_state()nearly_zeros() # 这将会影响rv_u的生成器v1 = f()rng = rv_u.rng.get_value(borrow=True)rng.set_state(state_after_v0)rv_u.rng.set_value(rng, borrow=True)v2 = f() # v2 != v1v3 = f() # v3 == v1 在Theano Graphs之间复制随机状态在很多应用场景中，使用者可能想把一个theano graph（图：g1,内置函数：f1）中的所有随机数生成器的状态传递给第二个theano graph（图：g2,内置函数：f2)。 例如：如果你试图从之前储存模型的参数中，初始化一个模型的状态，将会出现上述需要。theano.tensor.shared_randomstreams.RandomStreams和theano.sandbox.rng_mrg.MRG_RandomStreams这些在state_updates参数的复制元素可以实现。 每一次从RandomStreams对象中生成一个随机变量，将会有一个元组添加到state_update列表中。 第一个元素是共享变量：它表示和特定变量相关的随机数生成器的状态。第二个元素表示和随机数生成过程相对应的theano graph。 下面的例子展示了：随机状态(random states)如何从一个theano function 传递给另一个theano function中的。1234567891011121314151617181920212223242526272829303132333435import theanoimport numpyimport theano.tensor as Tfrom theano.sandbox.rng_mrg import MRG_RandomStreamsfrom theano.tensor.shared_randomstreams import RandomStreamsclass Graph: def __init__(self, seed=123): self.rng = RandomStreams(seed) self.y = self.rng.uniform(size=(1,))g1 = Graph(seed=123)f1 = theano.function([], g1.y)g2 = Graph(seed=987)f2 = theano.function([], g2.y)# 默认情况下，两个函数f1,f2不同步f1()f2()def copy_random_state(g1, g2): if isinstance(g1.rng, MRG_RandomStreams): g2.rng.rstate = g1.rng.rstate for (su1, su2) in zip(g1.rng.state_updates, g2.rng.state_updates): su2[0].set_value(su1[0].get_value())# 现在我们赋值theano随机数生成器的状态copy_random_state(g1, g2)f1()f2() 一个真实的例子：逻辑回归12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import numpyimport theanoimport theano.tensor as Trng = numpy.randomN = 400 # training sample sizefeats = 784 # number of input variables# generate a data set: D = (input_values, target_class)D = (rng.rand(N, feats), rng.randint(size=N, low=0, high=2))training_steps = 10000# Declare Theano symbolic variablesx = T.dmatrix(&apos;x&apos;)y = T.dvector(&apos;y&apos;)# initialize the weight vector w randomly## this and the following bias variable b# are shared so they keep their values# between training iterations (updates)w = theano.shared(rng.randn(feats), name=&apos;w&apos;)# initialize the bias termb = theano.shared(0., name=&apos;b&apos;)print(&apos;Initial model:&apos;)print(w.get_value())print(b.get_value())# Construct Theano expression graphp_1 = 1 / (1 + T.exp(-T.dot(x, w) - b)) # Probability that target = 1prediction = p_1 &gt; 0.5 # The prediction thresholdedxent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss functioncost = xent.mean() + 0.01 * (w ** 2).sum() # The cost to minimizegw, gb = T.grad(cost, [w, b]) # Compute the gradient of the cost# Compiletrain = theano.function( inputs=[x,y], outputs=[prediction, xent], updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))predict = theano.function(inputs=[x], outputs=prediction)# Trainfor i in range(training_steps): pred, err = train(D[0], D[1])print(&apos;Final model:&apos;)print(w.get_value())print(b.get_value())print(&apos;target values for D:&apos;)print(D[1])print(&apos;prediction on D:&apos;)print(predict(D[0]))","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - 代数","slug":"03-Theano-02","date":"2017-05-02T07:30:16.000Z","updated":"2017-07-17T15:29:21.661Z","comments":true,"path":"2017/05/02/03-Theano-02/","link":"","permalink":"http://yoursite.com/2017/05/02/03-Theano-02/","excerpt":"两个标量的加法为了让我们开始使用Theano，以及感受theano是如何工作的。接下来，我们构造一个简单的函数：加法。 ## 两个标量的加法1234567891011121314151617181920212223242526import numpyimport theano.tensor as Timport theanofrom theano import function# 定义两个符号（变量）x, y来表示你想实施加法的数。# 换句话说， x,y,z均为变量对象。# 在Theano中，所有的符号必须定义类型。# T.dscalar: 表示双精度（doubles)的0维数组（标量），他是Theano中的类型(Type)x = T.dscalar(&apos;x&apos;)y = T.dscalar(&apos;y&apos;)z = x + y# dscalar不是一个类(class)。因此，事实上x,y都不是dscalr的实例。# 它们是TensorVariable的实例。# 然而，x,y被赋值为theano的dscalar类型。type(x) # theano.tensor.var.TensorVariablex.type # TensorType(float64, scalar)T.dscalar # TensorType(float64, scalar)x.type is T.dscalar # True# 在你运行f时，你会注意到有些延迟# 因为f正在被编译为C代码f = function([x, y], z)f(2, 3)numpy.allclose(f(16.3, 12.1), 28.4)numpy.allcolse(z.eval(&#123;x: 16.3, y: 12.1&#125;), 28.4)","text":"两个标量的加法为了让我们开始使用Theano，以及感受theano是如何工作的。接下来，我们构造一个简单的函数：加法。 ## 两个标量的加法1234567891011121314151617181920212223242526import numpyimport theano.tensor as Timport theanofrom theano import function# 定义两个符号（变量）x, y来表示你想实施加法的数。# 换句话说， x,y,z均为变量对象。# 在Theano中，所有的符号必须定义类型。# T.dscalar: 表示双精度（doubles)的0维数组（标量），他是Theano中的类型(Type)x = T.dscalar(&apos;x&apos;)y = T.dscalar(&apos;y&apos;)z = x + y# dscalar不是一个类(class)。因此，事实上x,y都不是dscalr的实例。# 它们是TensorVariable的实例。# 然而，x,y被赋值为theano的dscalar类型。type(x) # theano.tensor.var.TensorVariablex.type # TensorType(float64, scalar)T.dscalar # TensorType(float64, scalar)x.type is T.dscalar # True# 在你运行f时，你会注意到有些延迟# 因为f正在被编译为C代码f = function([x, y], z)f(2, 3)numpy.allclose(f(16.3, 12.1), 28.4)numpy.allcolse(z.eval(&#123;x: 16.3, y: 12.1&#125;), 28.4) 两个矩阵的加法12345x = T.dmatrix(&apos;x&apos;)y = T.dmatrix(&apos;y&apos;)z = x + yf = function([x, y], z)f([[1, 2], [3, 4]], [[10, 20], [30, 40]]) 可以用到的类型(type)： byte: bscalar, bvector, bmatrix, brow, bcol, btensor3, btensor4, btensro5 16-bit intergers: wscalar, wvector, wmatrix, wrow, wcol, wtensor3, wtensor4, wtensor5 32-bit intergers: iscalar, ivector, imatrix, irow, icol, itensor3, itensor4, itensor5 64-bit intergers: lscalar, lvector, lmatrix, lrow, lcol, ltensor3, ltensor4, ltensor5 float: fscalar, fvector, fmatrix, frow, fcol, ftensor3, ftensor4, ftensor5 double: dscalar, dvector, dmatrix, drow, dcol, dtensor3, dtensor4, dtensor5 complex: cscalar, cvector, cmatrix, crow, ccol, ctensor3, ctensor4, ctensor5 练习1234a = theano.tensor.vector() # 声明一个变量out = a + a ** 10 # 构造一个符号表达式f = theano.function([a], out) # 编译一个函数print(f([0, 1, 2])) 修正并执行上面的代码，使得其能够计算：a ^ 2 + b ^ 2 + 2ab 12345678a = theano.tensor.vector()b = theano.tensor.vector()out1 = a ** 2 + b ** 2 + 2 * a * bout2 = (a + b) ** 2f1 = theano.function([a, b], out1)f2 = theano.function([a, b], out2)print(f1([0, 1], [1, 2]))print(f2([0, 1], [1, 2]))","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Theano - numpy新手","slug":"02-Theano-01","date":"2017-05-01T07:30:16.000Z","updated":"2017-07-17T15:28:41.576Z","comments":true,"path":"2017/05/01/02-Theano-01/","link":"","permalink":"http://yoursite.com/2017/05/01/02-Theano-01/","excerpt":"机器学习中的矩阵公约水平方向为矩阵的行，竖直方向为矩阵的列，每一行为一个样例（记录）。 因此输入[10, 5]表示：由10个样例（记录），每个样例具有5个维度（属性）组成的矩阵。如果[10,5]为一个神经网络的输入，那么权重矩阵的表示形式为[5, #hid]的矩阵。考虑一下数组：1234567import numpy as npnp.asarray([[1., 2], [3, 4], [5, 6]])np.asarray([[1., 2], [3, 4], [5, 6]]).shape# 这是一个3*2的矩阵，即有3行2列# 输出矩阵的第3行，第1列元素np.asarray([[1., 2], [3, 4], [5, 6]])[2, 0]","text":"机器学习中的矩阵公约水平方向为矩阵的行，竖直方向为矩阵的列，每一行为一个样例（记录）。 因此输入[10, 5]表示：由10个样例（记录），每个样例具有5个维度（属性）组成的矩阵。如果[10,5]为一个神经网络的输入，那么权重矩阵的表示形式为[5, #hid]的矩阵。考虑一下数组：1234567import numpy as npnp.asarray([[1., 2], [3, 4], [5, 6]])np.asarray([[1., 2], [3, 4], [5, 6]]).shape# 这是一个3*2的矩阵，即有3行2列# 输出矩阵的第3行，第1列元素np.asarray([[1., 2], [3, 4], [5, 6]])[2, 0] 广播(broadcasting)Numpy 在对不同形状的数组进行数学运算时进行广播。通俗的意思就是：较小的数组或标量将会被广播（扩展）形成一个较大尺寸的数组，得到相匹配的形状。例如：123456import numpy as npa = np.asarray([1.0, 2.0, 3.0])b = 2.0a * b # array([2., 4., 6.])# 较小尺寸的b在a*b运算期间，被扩展为和a同样尺寸的数组array([2., 2., 2.])# 这样极大地简化了b的书写，用标量2.0代替array([2.0, 2.0, 2.0])。","categories":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"Theano","slug":"Theano","permalink":"http://yoursite.com/tags/Theano/"}]},{"title":"Advanced R - 数据结构","slug":"01-advanced-r-data-structure","date":"2017-04-25T07:30:16.000Z","updated":"2017-07-17T15:28:30.084Z","comments":true,"path":"2017/04/25/01-advanced-r-data-structure/","link":"","permalink":"http://yoursite.com/2017/04/25/01-advanced-r-data-structure/","excerpt":"[译] Advanced R by Hadley Wickham 数据结构本章主要总结base R中最重要的数据结构。你之前很有可能已经使用到它们，或者它们中的一部分，但是可能从来没有用心思考过它们之间有什么关联。这里，我们将不会更深地去讨论单个数据结构的类型。而是，展示它们是如何由个体构成整体的。如果，你想了解更多的细节，请参照R语言官方文档。R的基础数据结构可以按照维度和同构性进行划分： 维度：1维、2维或者n维 同构性：某一数据结构中的数据类型是否要求一致","text":"[译] Advanced R by Hadley Wickham 数据结构本章主要总结base R中最重要的数据结构。你之前很有可能已经使用到它们，或者它们中的一部分，但是可能从来没有用心思考过它们之间有什么关联。这里，我们将不会更深地去讨论单个数据结构的类型。而是，展示它们是如何由个体构成整体的。如果，你想了解更多的细节，请参照R语言官方文档。R的基础数据结构可以按照维度和同构性进行划分： 维度：1维、2维或者n维 同构性：某一数据结构中的数据类型是否要求一致 下面给出了5种数据分析中最常用的数据结构：|# |同构 |异构 ||——|————————|——————–||1维 |原子向量 (Atomic vector)|列表 (List) ||2维 |矩阵 (Matrix) |数据框 (Data frame) ||n维 |数组 (Array) |- |几乎所有的对象都是建立在这几种数据结构基础之上的。在面向对象领域指南中（OO field guide）你将会看到这些简单的基础数据结构是如何构建起更加复杂的对象的。请注意：R没有0维数据结构，或者标量。你认为是标量的单个数字或者字符串，实际上是长度为1的向量。 给定一个对象，理解它是由哪些数据结构组成的最好方法是使用str()函数。str()是structure的缩写，它输出任何R数据结构的概括描述，而且这些描述是简洁易读的。 小测试利用这些小测试来决定你是否有必要阅读本章内容。如果你能立即想到答案，那么你可以放心地跳过本章。你可以在这里检验你的答案。 除了所包含的内容，向量（vector）的三个属性是什么？ 原子向量(atomic vector)的四种常见类型是什么？两种罕见类型是什么？ 属性（attributes）是什么？如何获取属性和设置属性？ 列表（list）和原子向量（atomic vector）的区别是什么？矩阵（matrix）和数据框（data frame）的区别是什么？ 列表（list）可以是矩阵（matirx）吗？矩阵（matrix）可以作为数据框（data frame）的一个列吗？ 提纲 向量（Vectors）介绍原子向量和列表，R的1维数据结构。 属性（Attributes）绕了一个小小的弯路去讨论属性，R的灵活的元数据规范。这里，你将会了解到因子（factors），通过设置原子向量的属性而得到的一种重要的数据结构。 矩阵和数组（Matrix and array）介绍矩阵和数组，储存2维和高维数据的数据结构。 数据框（Data frames）介绍数据框，R中储存数据的最重要的数据结构。数据框结合了列表和矩阵的行为，非常适合统计数据的需求。向量（Vectors）R语言中最基础的数据结构是向量。向量可以分为：原子向量和列表。它们有3个共同的属性： 类型，typeof()，它是什么。 长度，length()，它包含多少个元素。 属性，attributes()，附加的任意元数据。 它们在元素类型上有所区别：原子向量的所有元素必须是相同类型的，而列表的元素可以是不同类型的。 附注：is.vector()不能验证一个对象是否是向量。相反，只要对象是除了名称之外不含有任何属性的向量（原子向量和列表），它就返回TRUE。可以用is.atomic(x) || is.list(x)去验证一个对象是否真的是向量。 原子向量（Atomic vector）在这，我们将详细讨论原子向量的四种常见类型：logical, integer, double (常称为 numeric), 以及character。另外，还有两个罕见的类型将不做讨论：complex和raw。原子向量通常用c()创建，combine的缩写。123456dbl_var &lt;- c(1, 2.5, 4.5)# 使用L后缀, 你将会得到integer型，而并非double型int_var &lt;- c(1L, 6L, 10L)# 使用TRUE和FALSE (或T和F)创建逻辑向量log_var &lt;- c(TRUE, FALSE, T, F)chr_var &lt;- c(&quot;these are&quot;, &quot;some strings&quot;) 原子向量总是水平排列的，即使你使用c()进行嵌套：12345c(1, c(2, c(3, 4)))#&gt; [1] 1 2 3 4# 和下面方法相同c(1, 2, 3, 4)#&gt; [1] 1 2 3 4 注：上面创建的原子向量为列向量。 缺失值用NA来指定，长度为1的逻辑向量。在c()中NA将被强制转换为正确的类型，或者你可以使用NAreal (双精度浮点型向量), NAinteger 以及NAcharacter来创建特定类型的缺失值。 类型及测试给定一个向量，你可以使用typeof()来确定其类型，或者使用”is”函数验证它是否是某种特定的类型：is.character(), is.double(), is.integer(), is.logical(), 或更一般的 is.atomic()。 1234567891011121314int_var &lt;- c(1L, 6L, 10L)typeof(int_var)#&gt; [1] &quot;integer&quot;is.integer(int_var)#&gt; [1] TRUEis.atomic(int_var)#&gt; [1] TRUEdbl_var &lt;- c(1, 2.5, 4.5)typeof(dbl_var)#&gt; [1] &quot;double&quot;is.double(dbl_var)#&gt; [1] TRUEis.atomic(dbl_var)#&gt; [1] TRUE 附注：is.numeric()是对向量是否是“数值”类型一般的检验方法，它对整型（double),双精度浮点型（double）的向量都返回TRUE。它并不是针对双精度浮点型（double）向量的检验方法，我们通常称双精度浮点型（double）为“数值型”。1234is.numeric(int_var)#&gt; [1] TRUEis.numeric(dbl_var)#&gt; [1] TRUE 强制转换原子向量中的所有元素必须是同种类型的。所以，当你尝试合并多种类型数据时，将强制转换为最灵活的那种类型。灵活度从小到大依次为：逻辑型（logical）, 整型（integer）, 双精度浮点型（double）, 字符型（character）。 例如，合并字符型和整型输出字符型：12str(c(&quot;a&quot;, 1))#&gt; chr [1:2] &quot;a&quot; &quot;1&quot; 当逻辑型（logical）强制转换为整型（integer）和双精度浮点型（double）时，TRUE将会转换为1，FALSE转换为0。这在结合sum()和mean()使用时将会非常有用。123456789x &lt;- c(FALSE, FALSE, TRUE)as.numeric(x)#&gt; [1] 0 0 1# TRUE的总数sum(x)#&gt; [1] 1# TRUE所占的比例mean(x)#&gt; [1] 0.3333333 强制转换经常自动执行。大部分数学函数（+，log，abs等。)将强制转换为双精度浮点型（double）或者整型（integer），大部分逻辑运算符（&amp;，|，any等）将强制转换为逻辑型（logical）。如果强制过程中共有信息的丢失，你将会得到警告信息。如果转换遇到歧义，可以使用as.character(), as.double(), as.integer(), 或者 as.logical()进行明确的转换。 列表（Lists）列表不同于原子向量，因为它们的元素可以是任何类型的，包括列表类型。使用list()，而不是c()来创建列表。1234567x &lt;- list(1:3, &quot;a&quot;, c(TRUE, FALSE, TRUE), c(2.3, 5.9))str(x)#&gt; List of 4#&gt; $ : int [1:3] 1 2 3#&gt; $ : chr &quot;a&quot;#&gt; $ : logi [1:3] TRUE FALSE TRUE#&gt; $ : num [1:2] 2.3 5.9 列表有时被称为递归向量，因为一个列表可以包含其它表。这使它从根本上不同于原子向量。12345678x &lt;- list(list(list(list())))str(x)#&gt; List of 1#&gt; $ :List of 1#&gt; ..$ :List of 1#&gt; .. ..$ : list()is.recursive(x)#&gt; [1] TRUE c()可以将多个列表合并成一个列表。如果给定一个原子向量和列表组合，c()将会在合并它们之前把原子向量强制转换为列表。比较list()和c()两种结果：12345678910111213141516171819202122232425262728293031323334353637x &lt;- list(list(1, 2), c(3, 4))y &lt;- c(list(1, 2), c(3, 4))x#&gt; [[1]]#&gt; [[1]][[1]]#&gt; [1] 1#&gt; [[1]][[2]]#&gt; [1] 2#&gt; [[2]]#&gt; [1] 3 4str(x)#&gt; List of 2#&gt; $ :List of 2#&gt; ..$ : num 1#&gt; ..$ : num 2#&gt; $ : num [1:2] 3 4y#&gt; [[1]]#&gt; [1] 1#&gt; [[2]]#&gt; [1] 2#&gt; [[3]]#&gt; [1] 3#&gt; [[4]]#&gt; [1] 4str(y)#&gt; List of 4#&gt; $ : num 1#&gt; $ : num 2#&gt; $ : num 3#&gt; $ : num 4 对列表调用typeof()函数，将会输出列表。你可以使用is.list()来验证一个对象是否为列表，使用as.list()把一个对象强制转换为列表。你也可以使用unlist()将列表转换为原子向量。如果一个列表中的元素是不同数据类型的，unlist()使用与c()相同的强制规则，根据灵活度对数据进行强制转换。列表用来创建R语言中较为复杂的数据结构。例如：数据框（data frames)和线性模型对象（由lm()产生）都是列表。12345is.list(mtcars)#&gt; [1] TRUEmod &lt;- lm(mpg ~ wt, data = mtcars)is.list(mod)#&gt; [1] TRUE 练习 原子向量的6种类型是什么？列表和原子向量的区别是什么？ is.vector()和is.numeric()与is.list()和is.character()的根本区别是什么？ 检测你的向量强制转换规则知识的了解：尝试预测下面c()函数的强制输出结果： eval=FALSE&#125;1234c(1, FALSE)c(&quot;a&quot;, 1)c(list(1), &quot;a&quot;)c(TRUE, 1L) 为什么要用unlist()将列表转换为原子向量，而不是用as.vector()？ 为什么 1 == “1” 输出TRUE？ -1 &lt; FALSE 输出TRUE？ “one” &lt; 2 输出FALSE？ 为什么默认的缺失值NA是逻辑向量？逻辑向量有什么特殊地方？（提示：思考一下c(FALSE, NAcharacter)。属性（Attributes）所有的对象都可以拥有任意的附加属性，用于存储与对象相关的元数据。属性可以看做为具有唯一标识符的命名列表。属性可以通过attr()单独访问，也可以通过attributes()以列表的形式一次性访问。1234567y &lt;- 1:10attr(y, &quot;my_attribute&quot;) &lt;- &quot;This is a vector&quot;attr(y, &quot;my_attribute&quot;)#&gt; [1] &quot;This is a vector&quot;str(attributes(y))#&gt; List of 1#&gt; $ my_attribute: chr &quot;This is a vector&quot; structure()函数返回一个已被修改属性的新对象：1234structure(1:10, my_attribute = &quot;This is a vector&quot;)#&gt; [1] 1 2 3 4 5 6 7 8 9 10#&gt; attr(,&quot;my_attribute&quot;)#&gt; [1] &quot;This is a vector&quot; 默认情况下，当修改一个向量时，大多数属性都会丢失：1234attributes(y[1])#&gt; NULLattributes(sum(y))#&gt; NULL 但是，有三个重要的属性不会丢失： 名字（Names），一个字符型向量，为每一个元素赋予一个名字，将在 名字（Names）中描述。 维度（Dimensions），用于把向量转换为矩阵和数组，将在 矩阵和数组（Matrix and arrays）中描述。 类（Class），用于实现S3对象系统，将在S3中描述。这些属性中的任一属性都可以通过指定的函数进行访问和赋值。当访问这些属性的时候，使用names(x), dim(x), 和class(x), 而不是attr(x, “dim”), 和attr(x, “class”)。名字（Names）你可以通过三种方式为向量命名： 创建时命名：x \\&lt;- c(a = 1, b = 2, c = 3)。 修改现有的向量：x \\&lt;- 1:3; names(x) \\&lt;- c(“a”, “b”, “c”)。 创建一个向量的修订副本：x \\&lt;- setNames(1:3, c(“a”, “b”, “c”))。名字（Names）不是必须唯一的。然而，使用名字（Names）的最重要的原因是对字符取子集操作，当名字（Names）唯一时，取子集操作会更加有用。并不是向量中的所有元素都需要拥有一个名字。如果元素的名字缺失，names()将为这些元素返回空字符串。如果所有的名字缺失，names()将返回空值NULL。123456y &lt;- c(a = 1, 2, 3)names(y)#&gt; [1] &quot;a&quot; &quot;&quot; &quot;&quot;z &lt;- c(1, 2, 3)names(z)#&gt; NULL 你可以使用unname(x)创建一个没有名字的向量，或者通过 names(x) \\&lt;- NULL移除名字。 因子（Factors）属性的一个重要用途是定义因子。因子是只包含预定义值的向量，用来存储分类数据。因子建立在整型向量基础之上，拥有两个属性： class()：”factor”,这使得因子和常规的整型向量有所差别 levels(),定义了允许取值的集合 123456789101112131415161718x &lt;- factor(c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;))x#&gt; [1] a b b a#&gt; Levels: a bclass(x)#&gt; [1] &quot;factor&quot;levels(x)#&gt; [1] &quot;a&quot; &quot;b&quot;# You can&apos;t use values that are not in the levelsx[2] &lt;- &quot;c&quot;#&gt; Warning in `[&lt;-.factor`(`*tmp*`, 2, value = &quot;c&quot;): invalid factor level, NA#&gt; generatedx#&gt; [1] a &lt;NA&gt; b a #&gt; Levels: a b# 注: 你不可以合并因子c(factor(&quot;a&quot;), factor(&quot;b&quot;))#&gt; [1] 1 1 当你知道变量的所有可能取值时，即使你在数据集中看不到这些取值，因子是非常有用的。使用因子而不是字符向量可以使得没有包含这些取值的分组看起来更加醒目。1234567891011sex_char &lt;- c(&quot;m&quot;, &quot;m&quot;, &quot;m&quot;)sex_factor &lt;- factor(sex_char, levels = c(&quot;m&quot;, &quot;f&quot;))table(sex_char)#&gt; sex_char#&gt; m #&gt; 3table(sex_factor)#&gt; sex_factor#&gt; m f #&gt; 3 0 有时，你从文件中直接读取数据框时，那些你认为将产生数值型向量的列却变成了因子向量。这是由该列中含有非数值型的数据造成的，通常是用来标记缺失值的特殊符号：.或者-。为了纠正这种情况，可以先把因子向量强制转换为字符向量，然后再把字符向量转换为双精度浮点型向量。（这个过程后，务必检查缺失值。）当然，更好的方法是首先弄清问题产生的原因，然后及时解决；使用read.csv()的na.string参数通常是一个好的解决方法。1234567891011121314151617181920212223242526272829# 在这里从&quot;text&quot;中读取数据，而不是从文件中读取:z &lt;- read.csv(text = &quot;value\\n12\\n1\\n.\\n9&quot;)z#&gt; value#&gt; 1 12#&gt; 2 1#&gt; 3 .#&gt; 4 9typeof(z$value)#&gt; [1] &quot;integer&quot;as.double(z$value)#&gt; [1] 3 2 1 4# 哎呦, 不对: 3 2 1 4 是因子的水平, # 并不是我们所读取的数据。class(z$value)#&gt; [1] &quot;factor&quot;# 我们现在可以对它进行处理：as.double(as.character(z$value))#&gt; Warning: NAs introduced by coercion#&gt; [1] 12 1 NA 9# 或者改变我们的读取方式:z &lt;- read.csv(text = &quot;value\\n12\\n1\\n.\\n9&quot;, na.strings=&quot;.&quot;)typeof(z$value)#&gt; [1] &quot;integer&quot;class(z$value)#&gt; [1] &quot;integer&quot;z$value#&gt; [1] 12 1 NA 9# Perfect! :) 不幸的是，R语言中，大部分的数据加载函数都会自动地将字符型向量转换为因子向量。这种方法是欠佳的，因为这些函数并没有方法获取因子的所有水平，以及这些因子的最优次序。然而，我们可以通过设置参数 stringAsFactors = FALSE 阻止自动因子转换行为，然后再根据你对数据的了解，手工地将字符向量转换为因子向量。全局设置，options(stringsAsFactors = FALSE)也能控制自动因子转换行为，但不建议使用这种方法。因为修改全局设置后，当你运行其他代码（不管是包还是用source()引入代码），你会得到意想不到的结果。修改全局设置会使代码变得更难理解，因为这增加了你需要阅读的代码量，而且你必须得弄清楚这些代码起到什么样的作用。尽管因子向量看起来像，而且经常表现的像字符向量，它们实际上是整型向量。当把它们当做字符串处理的时候要多加小心。一些字符串处理方法（如gsub()和grepl()）将把因子强制转换为字符串，然而其它方法（如nchar()）将会抛出异常，以及其它方法（如c()）将会使用它们隐藏的整型数值（因子水平）。鉴于这些原因，如果你想让因子表现出类似字符串的行为，最好的方法就是把因子显式转换为字符向量。在R的早期版本中，使用因子向量比字符串向量有节省内存的优势，但是现在内存已不再是问题了。 练习 早期使用以下代码来举例说明structure()： 12structure(1:5, comment = &quot;my attribute&quot;)#&gt; [1] 1 2 3 4 5 但是，当打印对象时，你却看不到comment属性。这是为什么呢？是属性缺失？还是有什么特殊之处？（提示：尝试使用帮助文档。） 当你修改因子水平时，发生了什么？ results = \"none\"&#125;12f1 &lt;- factor(letters)levels(f1) &lt;- rev(levels(f1)) 这段代码有什么作用？f2,f3与f1有什么区别？ results = \"none\"&#125;123f2 &lt;- rev(factor(letters))f3 &lt;- factor(letters, levels = rev(letters)) 矩阵和数组（Matrices and arrays）正在更新……. Hadley Wickham 是 RStudio 的首席科学家以及 Rice University 统计系的助理教授。他是著名图形可视化软件包 ggplot2 的开发者，以及其他许多被广泛使用的软件包的作者，代表作品如 plyr、reshape2 等。文章英文原著来自于 Hadley Wickham 的Advanced R。","categories":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/categories/R语言/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R语言/"}]}]}