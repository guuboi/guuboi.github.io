{"meta":{"title":"League of Data","subtitle":"正确的前进方向取决于你当前所在的位置","description":null,"author":"Xiao蜗牛","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2017-07-13T15:07:37.000Z","updated":"2017-07-13T15:11:50.432Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-07-13T15:05:02.000Z","updated":"2017-07-13T15:12:45.481Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-07-13T15:07:18.000Z","updated":"2017-07-13T15:12:14.966Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2017-07-13T12:27:20.435Z","updated":"2017-07-13T12:27:20.435Z","comments":true,"path":"2017/07/13/hello-world/","link":"","permalink":"http://yoursite.com/2017/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"基于gensim的Wiki百科中文word2vec训练","slug":"01-word2vec-based-on-gensim","date":"2017-07-13T07:30:16.000Z","updated":"2017-07-13T15:42:32.224Z","comments":true,"path":"2017/07/13/01-word2vec-based-on-gensim/","link":"","permalink":"http://yoursite.com/2017/07/13/01-word2vec-based-on-gensim/","excerpt":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。","text":"Word2Vec简介Word2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。 我们怎么得到一个词的word2vec呢？下面我们将介绍如何使用python gensim得到我们想要的词向量。总的来说，包括以下几个步骤： wiki中文数据预处理 文本数据分词 gensim word2vec训练 wiki中文数据预处理首先，下载wiki中文数据：zhwiki-latest-pages-articles.xml.bz2。因为zhwiki数据中包含很多繁体字，所以我们想获得简体语料库，接下来需要做以下两件事： 使用gensim模块中的WikiCorpus从bz2中获取原始文本数据 使用OpenCC将繁体字转换为简体字 WikiCorpus获取原始文本数据数据处理的python代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940from __future__ import print_functionfrom gensim.corpora import WikiCorpusimport jiebaimport codecsimport osimport sixfrom gensim.models import Word2Vecfrom gensim.models.word2vec import LineSentenceimport multiprocessing class Config: data_path = &apos;xxx/zhwiki&apos; zhwiki_bz2 = &apos;zhwiki-latest-pages-articles.xml.bz2&apos; zhwiki_raw = &apos;zhwiki_raw.txt&apos; zhwiki_raw_t2s = &apos;zhwiki_raw_t2s.txt&apos; zhwiki_seg_t2s = &apos;zhwiki_seg.txt&apos; embedded_model_t2s = &apos;embedding_model_t2s/zhwiki_embedding_t2s.model&apos; embedded_vector_t2s = &apos;embedding_model_t2s/vector_t2s&apos; def dataprocess(_config): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw), &apos;w&apos;) wiki = WikiCorpus(os.path.join(_config.data_path, _config.zhwiki_bz2), lemmatize=False, dictionary=&#123;&#125;) for text in wiki.get_texts(): if six.PY3: output.write(b&apos; &apos;.join(text).decode(&apos;utf-8&apos;, &apos;ignore&apos;) + &apos;\\n&apos;) else: output.write(&apos; &apos;.join(text) + &apos;\\n&apos;) i += 1 if i % 10000 == 0: print(&apos;Saved &apos; + str(i) + &apos; articles&apos;) output.close() print(&apos;Finished Saved &apos; + str(i) + &apos; articles&apos;)config = Config()dataprocess(config) 使用OpenCC将繁体字转换为简体字这里，需要预先安装OpenCC，关于OpenCC在linux环境中的安装方法，请参考这篇文章。仅仅需要两行linux命令就可以完成繁体字转换为简体字的认为，而且速度很快。12$ cd /xxx/zhwiki/$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json 文本数据分词对于分词这个任务，我们直接使用了python的jieba分词模块。你也可以使用哈工大的ltp或者斯坦福的nltk python接口进行分词，准确率及权威度挺高的。不过这两个安装的时候会花费很长时间，尤其是斯坦福的。关于jieba的分词处理代码，参考如下：1234567891011121314151617181920212223242526272829303132def is_alpha(tok): try: return tok.encode(&apos;ascii&apos;).isalpha() except UnicodeEncodeError: return Falsedef zhwiki_segment(_config, remove_alpha=True): i = 0 if six.PY3: output = open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), &apos;w&apos;, encoding=&apos;utf-8&apos;) print(&apos;Start...&apos;) with codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw_t2s), &apos;r&apos;, encoding=&apos;utf-8&apos;) as raw_input: for line in raw_input.readlines(): line = line.strip() i += 1 print(&apos;line &apos; + str(i)) text = line.split() if True: text = [w for w in text if not is_alpha(w)] word_cut_seed = [jieba.cut(t) for t in text] tmp = &apos;&apos; for sent in word_cut_seed: for tok in sent: tmp += tok + &apos; &apos; tmp = tmp.strip() if tmp: output.write(tmp + &apos;\\n&apos;) output.close()zhwiki_segment(config) gensim word2vec训练python的gensim模块提供了word2vec训练，为我们模型的训练提供了很大的方便。关于gensim的使用方法，可以参考基于Gensim的Word2Vec实践。本次训练的词向量大小size为50，训练窗口为5，最小词频为5，并使用了多线程，具体代码如下：123456789101112131415161718192021def word2vec(_config, saved=False): print(&apos;Start...&apos;) model = Word2Vec(LineSentence(os.path.join(_config.data_path, _config.zhwiki_seg_t2s)), size=50, window=5, min_count=5, workers=multiprocessing.cpu_count()) if saved: model.save(os.path.join(_config.data_path, _config.embedded_model_t2s)) model.save_word2vec_format(os.path.join(_config.data_path, _config.embedded_vector_t2s), binary=False) print(&quot;Finished!&quot;) return model def wordsimilarity(word, model): semi = &apos;&apos; try: semi = model.most_similar(word, topn=10) except KeyError: print(&apos;The word not in vocabulary!&apos;) for term in semi: print(&apos;%s,%s&apos; % (term[0],term[1]))model = word2vec(config, saved=True) word2vec训练已经完成，我们得到了想要的模型以及词向量，并保存到本地。下面我们分别查看同【宝马】和【腾讯】最相近的前10个词语。可以发现：和【宝马】相近的词大都属于汽车行业，而且是汽车品牌；和【腾讯】相近的词大都属于互联网行业。1234567891011121314151617181920212223&gt;&gt;&gt; wordsimilarity(word=u&apos;宝马&apos;, model=model)保时捷,0.92567974329固特异,0.888278841972劳斯莱斯,0.884045600891奥迪,0.881808757782马自达,0.881799697876亚菲特,0.880708634853欧宝,0.877104878426雪铁龙,0.876984715462玛莎拉蒂,0.868475496769桑塔纳,0.865387916565&gt;&gt;&gt; wordsimilarity(word=u&apos;腾讯&apos;, model=model)网易,0.880213916302优酷,0.873666107655腾讯网,0.87026232481广州日报,0.859486758709微信,0.835543811321天涯社区,0.834927380085李彦宏,0.832848489285土豆网,0.831390202045团购,0.829696238041搜狐网,0.825544642448 附相关数据及代码，包含：简体字转换后文本，分词后文本，以及50维word2vec词向量。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://yoursite.com/categories/自然语言处理/"}],"tags":[{"name":"gensim","slug":"gensim","permalink":"http://yoursite.com/tags/gensim/"},{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"},{"name":"wiki","slug":"wiki","permalink":"http://yoursite.com/tags/wiki/"}]}]}